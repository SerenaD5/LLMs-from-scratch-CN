{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f896245e-57c4-48fd-854f-9e43f22e10c9",
      "metadata": {
        "id": "f896245e-57c4-48fd-854f-9e43f22e10c9"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7fc8a0-280c-4979-b0c7-fc3a99b3b785",
      "metadata": {
        "id": "ca7fc8a0-280c-4979-b0c7-fc3a99b3b785"
      },
      "source": [
        "# 附录A: Pytorch介绍"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "许多科学计算库不会立即支持最新版本的 Python。因此，在安装 PyTorch 时，建议使用\n",
        "比最新版本旧一到两个版本的 Python。如果最新的 Python 版本是 Python 3.13，那么推荐使用\n",
        "Python 3.11 或 Python 3.12\n",
        "\n",
        "`pip install torch`\n",
        "\n",
        "假设你的计算机支持兼容 CUDA 的 GPU。在这种情况下，如果你正在使用的 Python 环境已安装\n",
        "必要的依赖项（如 pip），那么系统将自动安装支持 CUDA 加速的 PyTorch 版本。\n",
        "\n",
        "\n",
        "本书中使用的是 PyTorch 2.4.0，为了确保与本书的兼容性，建议你使用以下命令安装该版本：\n",
        "`pip install torch==2.4.0`\n",
        "\n",
        "建议你访问 PyTorch 官方网站并使用安装菜单选择适合你操作系统的安装命令"
      ],
      "metadata": {
        "id": "NGEuEV5PDzwF"
      },
      "id": "NGEuEV5PDzwF"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW1pJ7GfDgRx",
        "outputId": "8001e113-46fc-49ab-867f-3ba5e9fa765a"
      },
      "id": "lW1pJ7GfDgRx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5bf13d2-8fc2-483e-88cc-6b4310221e68",
      "metadata": {
        "id": "f5bf13d2-8fc2-483e-88cc-6b4310221e68"
      },
      "source": [
        "## A.1 什么是PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ee5660-5327-48e2-9104-a882b3b2afa4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ee5660-5327-48e2-9104-a882b3b2afa4",
        "outputId": "5410ea9c-3b66-4615-99e3-3b3b91cb98d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73ad4e4-7ec6-4467-a9e9-0cdf6d195264",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f73ad4e4-7ec6-4467-a9e9-0cdf6d195264",
        "outputId": "b5389214-cd07-483f-b7a6-7b06d296a050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397ba1ab-3306-4965-8618-1ed5f24fb939",
      "metadata": {
        "id": "397ba1ab-3306-4965-8618-1ed5f24fb939"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/1.png\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e3c0555-88f6-4515-8c99-aa56b0769d54",
      "metadata": {
        "id": "1e3c0555-88f6-4515-8c99-aa56b0769d54"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/2.png\" width=\"300px\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/3.png\" width=\"300px\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/4.png\" width=\"500px\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A1/5.png\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2100cf2e-7459-4ab3-92a8-43e86ab35a9b",
      "metadata": {
        "id": "2100cf2e-7459-4ab3-92a8-43e86ab35a9b"
      },
      "source": [
        "## A.2 理解张量"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c484e87-bfc9-4105-b0a7-1e23b2a72a30",
      "metadata": {
        "id": "3c484e87-bfc9-4105-b0a7-1e23b2a72a30"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A2/1.png\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch 采用了大部分 NumPy 数组 API 和语法来进行张量操作, 如果你对 NumPy 不熟悉，可以通过我的文章:\n",
        "Scientific Computing in Python: Introduction to NumPy and Matplotlib >>> https://sebastianraschka.com/blog/2020/numpy-intro.html"
      ],
      "metadata": {
        "id": "UTFKhkacIDnP"
      },
      "id": "UTFKhkacIDnP"
    },
    {
      "cell_type": "markdown",
      "id": "26d7f785-e048-42bc-9182-a556af6bb7f4",
      "metadata": {
        "id": "26d7f785-e048-42bc-9182-a556af6bb7f4"
      },
      "source": [
        "### A.2.1 标量、向量、矩阵和张量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a464d6-cec8-4363-87bd-ea4f900baced",
      "metadata": {
        "id": "a3a464d6-cec8-4363-87bd-ea4f900baced"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 从Python整数创建一个零维张量（标量）\n",
        "tensor0d = torch.tensor(1)\n",
        "\n",
        "# 从Python列表创建一个一维张量（向量）\n",
        "tensor1d = torch.tensor([1, 2, 3])\n",
        "\n",
        "# 从嵌套的Python列表创建一个二维张量\n",
        "tensor2d = torch.tensor([[1, 2],\n",
        "                         [3, 4]])\n",
        "\n",
        "# 从嵌套的Python列表创建一个三维张量\n",
        "tensor3d_1 = torch.tensor([[[1, 2], [3, 4]],\n",
        "                           [[5, 6], [7, 8]]])\n",
        "\n",
        "# 从NumPy数组创建一个张量\n",
        "ary3d = np.array([[[1, 2], [3, 4]],\n",
        "                  [[5, 6], [7, 8]]])\n",
        "tensor3d_2 = torch.tensor(ary3d)  # 复制 NumPy 数组\n",
        "tensor3d_3 = torch.from_numpy(ary3d)  # 与 NumPy 数组共享内存"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor0d)\n",
        "print(tensor1d)\n",
        "print(tensor2d)\n",
        "print(tensor3d_1)\n",
        "print(tensor3d_2)\n",
        "print(tensor3d_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj769KyWPtQ6",
        "outputId": "2c4c580e-3f8a-453e-de5e-4d8ac37bab9c"
      },
      "id": "mj769KyWPtQ6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1)\n",
            "tensor([1, 2, 3])\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe14c47-499a-4d48-b354-a0e6fd957872",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbe14c47-499a-4d48-b354-a0e6fd957872",
        "outputId": "4833a5e7-7403-4b2d-91dd-44af99e364fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n"
          ]
        }
      ],
      "source": [
        "ary3d[0, 0, 0] = 999\n",
        "print(tensor3d_2) # 保持不变"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e4c23a-cdba-46f5-a2dc-5fb32bf9117b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3e4c23a-cdba-46f5-a2dc-5fb32bf9117b",
        "outputId": "177b9359-efd6-4e73-d161-7c7d0ad92543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[999,   2],\n",
            "         [  3,   4]],\n",
            "\n",
            "        [[  5,   6],\n",
            "         [  7,   8]]])\n"
          ]
        }
      ],
      "source": [
        "print(tensor3d_3) # 由于共享内存而发生变化"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63dec48d-2b60-41a2-ac06-fef7e718605a",
      "metadata": {
        "id": "63dec48d-2b60-41a2-ac06-fef7e718605a"
      },
      "source": [
        "### A.2.2 张量数据类型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f48c014-e1a2-4a53-b5c5-125812d4034c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f48c014-e1a2-4a53-b5c5-125812d4034c",
        "outputId": "e3863709-3626-463c-fe7a-d809f8da47fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n"
          ]
        }
      ],
      "source": [
        "tensor1d = torch.tensor([1, 2, 3])\n",
        "print(tensor1d.dtype) #查看数据类型 PyTorch 采用 Python 默认的 64 位整数数据类型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5429a086-9de2-4ac7-9f14-d087a7507394",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5429a086-9de2-4ac7-9f14-d087a7507394",
        "outputId": "4af1ab1b-9dc1-4194-cf65-680c35ee6089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(floatvec.dtype) #如果使用 Python 浮点数创建张量，那么 PyTorch 默认会创建具有 32 位精度的张量, 这种选择主要是为了在精度和计算效率之间取得平衡, 1. 大多数深度学习任务里足够精度, 且内存和计算资源少 2. GPU架构对32bit有优化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a438d1-49bb-481c-8442-7cc2bb3dd4af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9a438d1-49bb-481c-8442-7cc2bb3dd4af",
        "outputId": "da992c28-d69e-49e4-9a26-45f4d40c2fb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "floatvec = tensor1d.to(torch.float32) # 可以使用张量的.to 方法更改精度\n",
        "print(floatvec.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2020deb5-aa02-4524-b311-c010f4ad27ff",
      "metadata": {
        "id": "2020deb5-aa02-4524-b311-c010f4ad27ff"
      },
      "source": [
        "### A.2.3 常见的PyTorch张量操作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02095f2-8a48-4953-b3c9-5313d4362ce7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c02095f2-8a48-4953-b3c9-5313d4362ce7",
        "outputId": "fbe3a5e5-5402-490a-b886-ff7c8db02c65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tensor2d = torch.tensor([[1, 2, 3],\n",
        "                         [4, 5, 6]])\n",
        "tensor2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f33e1d45-5b2c-4afe-b4b2-66ac4099fd1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f33e1d45-5b2c-4afe-b4b2-66ac4099fd1a",
        "outputId": "73f0f547-4e73-436d-ce45-c9aa930b7407"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tensor2d.shape # .shape 属性允许我们访问张量的形状"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a4129d-f870-4e03-9c32-cd8521cb83fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3a4129d-f870-4e03-9c32-cd8521cb83fe",
        "outputId": "72af63d5-0d08-48d0-ecb1-fc3241d8e100"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tensor2d.reshape(3, 2) #要将该张量变为 3×2 的形状"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589ac0a7-adc7-41f3-b721-155f580e9369",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "589ac0a7-adc7-41f3-b721-155f580e9369",
        "outputId": "9baad2cd-030a-445e-974c-73bd5fba34f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tensor2d.view(3, 2) #在 PyTorch 中，重塑张量更常用的命令是.view()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZh_T-DST9kh",
        "outputId": "1a5ee10c-5779-4732-b615-6f7185f1642a"
      },
      "id": "dZh_T-DST9kh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".view()和.reshape()的微妙区别在于它们对内存布局的处理方式：.view()要求原始数据是连续的，如果不是，它将无法工作，而.reshape()会工作，如有必要，它会复制数据以确保所需的形状"
      ],
      "metadata": {
        "id": "DwqE5gZVTw_Q"
      },
      "id": "DwqE5gZVTw_Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "344e307f-ba5d-4f9a-a791-2c75a3d1417e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "344e307f-ba5d-4f9a-a791-2c75a3d1417e",
        "outputId": "21220f84-a35f-4af9-c11b-d2a1f1907491"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 4],\n",
              "        [2, 5],\n",
              "        [3, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tensor2d.T #转置张量, 将其沿对角线翻转"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a75030-6a41-4ca8-9aae-c507ae79225c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19a75030-6a41-4ca8-9aae-c507ae79225c",
        "outputId": "3a1ce466-5ce0-4f02-9eb3-14e7aebde5d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14, 32],\n",
              "        [32, 77]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tensor2d.matmul(tensor2d.T) #PyTorch 中常用的矩阵相乘方法, 也可以使用@运算符，它能够更简洁地实现相同的功能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c950bc-d640-4203-b210-3ac8932fe4d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7c950bc-d640-4203-b210-3ac8932fe4d4",
        "outputId": "a05ae6e1-d3da-414c-bcac-681f2e40c7e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14, 32],\n",
              "        [32, 77]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tensor2d @ tensor2d.T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c15bdeb-78e2-4870-8a4f-a9f591666f38",
      "metadata": {
        "id": "4c15bdeb-78e2-4870-8a4f-a9f591666f38"
      },
      "source": [
        "## A.3 将模型视为计算图"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f3e16c3-07df-44b6-9106-a42fb24452a9",
      "metadata": {
        "id": "0f3e16c3-07df-44b6-9106-a42fb24452a9"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A3/1.png\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22af61e9-0443-4705-94d7-24c21add09c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22af61e9-0443-4705-94d7-24c21add09c7",
        "outputId": "5d0b2723-07c6-414d-a25e-fb12882adc89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0852)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "y = torch.tensor([1.0])  # 真实标签\n",
        "x1 = torch.tensor([1.1]) # 输入特征\n",
        "w1 = torch.tensor([2.2]) # 权重参数\n",
        "b = torch.tensor([0.0])  # 偏置单元\n",
        "\n",
        "z = x1 * w1 + b          # 净输入\n",
        "a = torch.sigmoid(z)     # 激活和输出\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9424f26-2bac-47e7-b834-92ece802247c",
      "metadata": {
        "id": "f9424f26-2bac-47e7-b834-92ece802247c"
      },
      "source": [
        "## A.4 轻松实现自动微分"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33aa2ee4-6f1d-448d-8707-67cd5278233c",
      "metadata": {
        "id": "33aa2ee4-6f1d-448d-8707-67cd5278233c"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A4/1.png\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf5cef7-48d6-4d2a-8ab0-0fb10bdd7d1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebf5cef7-48d6-4d2a-8ab0-0fb10bdd7d1a",
        "outputId": "8fdcd9f2-0094-40f9-a357-31431bc92250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([-0.0898]),)\n",
            "(tensor([-0.0817]),)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F #Imports the functional interface for neural network operations from PyTorch.\n",
        "from torch.autograd import grad #Imports the grad function specifically for calculating gradients.\n",
        "\n",
        "y = torch.tensor([1.0]) #Defines the true label as a PyTorch tensor with a value of 1.0.\n",
        "x1 = torch.tensor([1.1]) #Defines the input feature as a PyTorch tensor with a value of 1.1.\n",
        "w1 = torch.tensor([2.2], requires_grad=True) #Defines the weight parameter as a PyTorch tensor with a value of 2.2. requires_grad=True is crucial here, as it tells PyTorch to track operations on this tensor so that gradients can be computed later.\n",
        "b = torch.tensor([0.0], requires_grad=True) #Defines the bias term as a PyTorch tensor with a value of 0.0\n",
        "\n",
        "z = x1 * w1 + b  #Calculates the net input z by performing a linear transformation on the input feature x1 using the weight w1 and adding the bias b.\n",
        "a = torch.sigmoid(z) #Applies the sigmoid activation function to the net input z to get the output a\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y) #Calculates the binary cross-entropy loss between the output a and the true label y\n",
        "\n",
        "grad_L_w1 = grad(loss, w1, retain_graph=True) #默认情况下，PyTorch 在计算梯度后会销毁计算图以释放内存。然而，由于我们即将再次使用这个计算图，因此可以设置 retain_graph=True，使其保留在内存中\n",
        "grad_L_b = grad(loss, b, retain_graph=True)\n",
        "\n",
        "print(grad_L_w1)\n",
        "print(grad_L_b)\n",
        "#这里我们手动使用了 grad 函数，这在实验、调试和概念演示中很有用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c5875d-f6b2-492c-b5ef-7e132f93a4e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93c5875d-f6b2-492c-b5ef-7e132f93a4e0",
        "outputId": "6df547af-04b4-4f50-9f7d-46debf5c245f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0898])\n",
            "tensor([-0.0817])\n"
          ]
        }
      ],
      "source": [
        "# 在实际操作中，PyTorch 提供了更高级的工具来自动化这个过程。例如，我们可以对损失函数调用.backward方法，随后 PyTorch 将计算计算图中所有叶节点的梯度，这些梯度将通过张量的.grad 属性进行存储\n",
        "loss.backward()\n",
        "\n",
        "print(w1.grad)\n",
        "print(b.grad)\n",
        "\n",
        "#PyTorch 通过.backward方法为我们处理了微积分问题——我们不需要手动计算任何导数或梯度"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53bdd7d-44e6-40ab-8a5a-4eef74ef35dc",
      "metadata": {
        "id": "f53bdd7d-44e6-40ab-8a5a-4eef74ef35dc"
      },
      "source": [
        "## A.5 实现多层神经网络"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6cb9787-2bc8-4379-9e8c-a3401ac63c51",
      "metadata": {
        "id": "d6cb9787-2bc8-4379-9e8c-a3401ac63c51"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A5/1.png\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b749e1-7768-4cfe-94d6-a08c7feff4a1",
      "metadata": {
        "id": "84b749e1-7768-4cfe-94d6-a08c7feff4a1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "In summary, this class creates a simple feedforward neural network with two hidden layers\n",
        "using ReLU activation and an output layer. The number of input features and output units\n",
        "are customizable through the constructor.\n",
        "'''\n",
        "class NeuralNetwork(torch.nn.Module): #在 PyTorch 中实现神经网络时，可以通过子类化 torch.nn.Module 类来定义我们自己的自定义网络架构\n",
        "    def __init__(self, num_inputs, num_outputs): # 将输入和输出的数量编码为变量，使我们可以在具有不同特征数量和类别数量的数据集上重复使用相同的代码\n",
        "        super().__init__() #This line calls the constructor of the parent class (torch.nn.Module). This is necessary to properly initialize the module.\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_inputs, 30), #线性层将输入节点和输出节点的数量作为参数\n",
        "            torch.nn.ReLU(), #非线性激活函数被放置在隐藏层之间\n",
        "\n",
        "            # 2nd hidden layer\n",
        "            torch.nn.Linear(30, 20), #一个隐藏层的输出节点数量必须与下一层的输入节点数量相匹配\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(20, num_outputs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): #This method defines the forward pass of the neural network. It takes an input tensor x and passes it through the defined layers.\n",
        "        logits = self.layers(x)\n",
        "        return logits #最后一层的输出称为 logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b59e2e-1930-456d-93b9-f69263e3adbe",
      "metadata": {
        "id": "c5b59e2e-1930-456d-93b9-f69263e3adbe"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork(50, 3) #实例化一个新的神经网络对象\n",
        "#(50, 3) are the arguments passed to the __init__ method of the NeuralNetwork class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39d02a21-33e7-4879-8fd2-d6309faf2f8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39d02a21-33e7-4879-8fd2-d6309faf2f8d",
        "outputId": "6350de5e-32aa-4bb2-cccf-a1c6683c9447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "请注意，在实现 NeuralNetwork 类时，我们使用了 Sequential 类。虽然 Sequential 并非\n",
        "必需，但如果有一系列想要按特定顺序执行的层（正如本例中的情况），那么使用它可以让我们\n",
        "的工作更轻松。因此，在__init__构造函数中实例化 self.layers = Sequential(...)后，\n",
        "只需在 NeuralNetwork 的 forward 方法中调用 self.layers，而无须单独调用每个层。\n",
        "'''\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94535738-de02-4c2a-9b44-1cd186fa990a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94535738-de02-4c2a-9b44-1cd186fa990a",
        "outputId": "c93f0fd4-7019-42d2-8559-7e214b77e316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable model parameters: 2213\n"
          ]
        }
      ],
      "source": [
        "#检查一下该模型的可训练参数总数\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad) #每一个 requires_grad=True 的参数都会被视为可训练参数，并在训练期间进行更新\n",
        "print(\"Total number of trainable model parameters:\", num_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)`: This line calculates the sum of the number of elements (parameters) in all the layers of the `model` that require gradients.\n",
        "\n",
        "- `model.parameters()`: This method returns an iterator over all the parameters (weights and biases) of the model.\n",
        "- `for p in model.parameters()`: This iterates through each parameter p in the model.\n",
        "if p.requires_grad: This condition filters the parameters to include only those that have `requires_grad` set to `True`. These are the parameters that will be updated during the training process.\n",
        "- `p.numel()`: This method returns the total number of elements (scalars) in a given parameter tensor p.\n",
        "- `sum(...)`: This sums up the number of elements for all the parameters that meet the condition."
      ],
      "metadata": {
        "id": "pUWeO09XPnR5"
      },
      "id": "pUWeO09XPnR5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "对于前面我们提到的具有两个隐藏层的神经网络模型，这些可训练参数包含在 `torch.nn.Linear` 层中。`Linear` 层会将输入与权重矩阵相乘，并加上一个偏置向量。这有时被称为**前馈层**或**全连接层**\n",
        "\n",
        "基于这里执行的 `print(model)`调用，可以看到第一个 `Linear` 层在 `layers` 属性中的索引位置是 0。\n",
        "\n",
        "可以通过以下方式访问对应的权重参数矩阵"
      ],
      "metadata": {
        "id": "6liH_EfIQUhb"
      },
      "id": "6liH_EfIQUhb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c394106-ad71-4ccb-a3c9-9b60af3fa748",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c394106-ad71-4ccb-a3c9-9b60af3fa748",
        "outputId": "e15b4264-d4d7-4f12-a9b8-ee8a6e08fd6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.1388,  0.0159,  0.1215,  ...,  0.1032,  0.0296,  0.0102],\n",
            "        [ 0.0229,  0.0260, -0.0458,  ..., -0.0358,  0.0362,  0.0497],\n",
            "        [-0.0896,  0.0113,  0.1370,  ...,  0.1037,  0.1230, -0.0929],\n",
            "        ...,\n",
            "        [-0.1362, -0.0713, -0.0010,  ...,  0.1176,  0.1054, -0.1012],\n",
            "        [ 0.1226,  0.0937, -0.1409,  ...,  0.1321, -0.0613,  0.0086],\n",
            "        [-0.0045, -0.0604,  0.0535,  ...,  0.0697,  0.0373,  0.0923]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(model.layers[0].weight) #访问权重参数矩阵"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da9a35e-44f3-460c-90fe-304519736fd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1da9a35e-44f3-460c-90fe-304519736fd6",
        "outputId": "37b3783d-5eb5-477d-93fd-172b4da0e499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 50])\n"
          ]
        }
      ],
      "source": [
        "print(model.layers[0].weight.shape) #查看其维度\n",
        "#这里的权重矩阵是一个 30×50 的矩阵，可以看到 requires_grad 被设置为 True（意味着该矩阵是可训练的）——这是 torch.nn.Linear 中权重和偏置的默认设置"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layers[0].bias) # 访问偏置向量"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBA6b40KRAuO",
        "outputId": "1bb636f7-487c-4755-94a0-733fa5faa5f4"
      },
      "id": "jBA6b40KRAuO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([-0.0181,  0.1404,  0.0374,  0.1102,  0.0045,  0.0788,  0.1013,  0.0211,\n",
            "         0.1191, -0.1204, -0.0152, -0.0222, -0.0056,  0.0466, -0.0365, -0.0321,\n",
            "         0.0927, -0.1029, -0.0093,  0.1047,  0.1279, -0.1176,  0.0445,  0.0583,\n",
            "         0.0263,  0.0459, -0.0549,  0.0258,  0.0305,  0.0463],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layers[0].bias.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOstVfS7RZjd",
        "outputId": "182db6c0-5e2d-4e2b-a35e-269fd7600cde"
      },
      "id": "xOstVfS7RZjd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果你在自己的计算机上执行前面的代码，那么权重矩阵中的数值可能会与本书展示的有所不同。\n",
        "\n",
        "模型权重会用小的随机数进行初始化，每次实例化网络时这些数值都会不同。在深度学习中，使用小的随机数初始化模型权重是为了在训练过程中打破对称性。否则，各节点将执行相同的操作并在反向传播过程中进行相同的更新，导致网络无法学习从输入到输出的复杂映射关系。"
      ],
      "metadata": {
        "id": "SA3h_bNcR8PJ"
      },
      "id": "SA3h_bNcR8PJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b201882b-9285-4db9-bb63-43afe6a2ff9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b201882b-9285-4db9-bb63-43afe6a2ff9e",
        "outputId": "6d5b5ffa-1b24-4346-be13-22ea20752315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
            "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
            "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
            "        ...,\n",
            "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
            "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
            "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123) #可以通过 manual_seed 来为 PyTorch 的随机数生成器设定种子，从而使随机数初始化可重复\n",
        "\n",
        "model = NeuralNetwork(50, 3)\n",
        "print(model.layers[0].weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57eadbae-90fe-43a3-a33f-c23a095ba42a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57eadbae-90fe-43a3-a33f-c23a095ba42a",
        "outputId": "005deed2-c63e-4d6a-a1de-13da0b5bac81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This code snippet demonstrates how to perform a forward pass through the model\n",
        "with a single input sample and how to prevent gradient calculation during inference.\n",
        "'''\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "X = torch.rand((1, 50)) #This line creates a random tensor named X with a shape of (1, 50).\n",
        "# torch.rand() creates a tensor filled with random numbers from a uniform distribution between 0 and 1.\n",
        "# (1, 50) specifies the shape of the tensor. A shape of (1, 50) represents a single input sample with 50 features, which matches the num_inputs specified when the NeuralNetwork was instantiated earlier.\n",
        "\n",
        "out = model(X) #This line performs a forward pass through the model using the input tensor X\n",
        "#When you call a torch.nn.Module instance like a function (model(X)), it automatically executes the forward method you defined in the class. The output of the forward pass (the logits in this case) is stored in the out variable.\n",
        "print(out)\n",
        "\n",
        "#You'll notice that the output tensor has grad_fn=<AddmmBackward0>, indicating that PyTorch is tracking the operations performed to compute out for potential gradient calculations later."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output `tensor([[-0.1262, 0.1080, -0.1792]], grad_fn=<AddmmBackward0>)` represents the **logits** produced by the neural network for the single input sample.\n",
        "\n",
        "Here's what each part means:\n",
        "\n",
        "`tensor([[-0.1262, 0.1080, -0.1792]])`: This is the actual output data. It's a tensor with **one row and three columns**. Since the NeuralNetwork was instantiated with num_outputs=3, each of these three values **corresponds to the raw, unnormalized score (or logit) **for each of the three possible output classes. A higher logit value for a class generally indicates that the model has higher confidence that the input belongs to that class.\n",
        "`grad_fn=<AddmmBackward0`>: This part indicates that this tensor is the result of an operation (`AddmmBackward0` corresponds to a matrix multiplication followed by an addition, which is what happens in a linear layer), and that PyTorch is tracking the operations performed to create this tensor. This tracking is essential for automatic differentiation, allowing PyTorch to compute gradients during the backward pass for training.\n",
        "\n",
        "In a typical classification task, these logits would be passed through a softmax function to convert them into probabilities for each class, which sum up to 1.\n",
        "\n",
        "结果中返回的 3 个数值对应于分配给每个输出节点的分数。注意输出张量还包含了一个grad_fn 值.\n",
        "\n",
        "`grad_fn=<AddmmBackward0>`意味着我们正在查看的张量是通过矩阵乘法和加法操作创建的。PyTorch 会在反向传播期间使用这些信息来计算梯度。`grad_fn=<AddmmBackward0>`中的\n",
        "`<AddmmBackward0>`指定了执行的操作。在这种情况下，它执行的是一个 Addmm 操作。Addmm代表的是矩阵乘法（mm）后接加法（Add）的组合运算"
      ],
      "metadata": {
        "id": "ABK4EZiuX7Tc"
      },
      "id": "ABK4EZiuX7Tc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d720cb-ef73-4b7b-92e0-8198a072defd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48d720cb-ef73-4b7b-92e0-8198a072defd",
        "outputId": "5a9e24c9-8351-4962-e6bb-2da636117d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1262,  0.1080, -0.1792]])\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "如果只想使用网络进行预测而不进行训练或反向传播（比如在训练之后使用它进行预测），\n",
        "那么为反向传播构建这个计算图可能会浪费资源，因为它会执行不必要的计算并消耗额外的内\n",
        "存。因此，当使用模型进行推理（比如做出预测）而不是训练时，最好的做法是使用\n",
        "torch.no_grad()上下文管理器。这会告诉 PyTorch 无须跟踪梯度，从而可以显著节省内存和\n",
        "计算资源\n",
        "'''\n",
        "with torch.no_grad():\n",
        "    out = model(X)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10df3640-83c3-4061-a74d-08f07a5cc6ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10df3640-83c3-4061-a74d-08f07a5cc6ac",
        "outputId": "27aa7e13-628c-4313-a225-d7e2e0ce621c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3113, 0.3934, 0.2952]])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    out = torch.softmax(model(X), dim=1) #dim=1: This specifies the dimension along which the softmax function is applied. In this case, dim=1 means the softmax is applied across the columns (the different output classes) for each input sample (row). This is the standard way to apply softmax for classification tasks where each row represents a single instance and each column represents a class score.\n",
        "print(out) # 输出的结果现在这些值可以解释为类别成员的概率，并且它们的总和大约为 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**模型通常返回 Logits:** 在 PyTorch 中构建分类模型时，习惯上让模型的最后一层只进行线性变换，输出的是原始的、未经过 Softmax 或 Sigmoid 激活的数值，这些数值被称为 logits。\n",
        "损失函数中集成了 Softmax/Sigmoid: PyTorch 提供了一些常用的损失函数，例如用于多分类的 CrossEntropyLoss 和用于二分类的 BCEWithLogitsLoss。这些损失函数在内部已经集成了 Softmax（或 Sigmoid）操作和负对数似然损失计算。\n",
        "\n",
        "**原因：** 数值计算的效率和稳定性: 将 Softmax/Sigmoid 和损失函数结合在一起计算，相比于先单独计算 Softmax/Sigmoid 再计算损失，在数值计算上更有效率，并且可以提高数值稳定性。这是因为 Softmax/Sigmoid 的计算涉及到指数运算，当输入值很大或很小时，可能会出现数值溢出或下溢的问题。将它们与对数似然损失结合后，可以通过一些数学技巧（例如 LogSumExp 技巧）来避免这些问题，从而提高计算的精度和稳定性。\n",
        "\n",
        "**需要概率时再显式调用:** 因此，如果您只是想在训练过程中计算损失并进行反向传播，您可以直接将模型的 logits 输出传递给 PyTorch 集成了 Softmax/Sigmoid 的损失函数。只有当您需要查看模型对每个类别的预测概率时（例如在推理阶段或者进行模型评估时），才需要显式地对模型的 logits 输出应用 Softmax 函数，就像您在代码中看到的那样。\n",
        "\n",
        "总结来说，这种做法是为了在训练过程中获得更好的数值表现，同时仍然允许您在需要时方便地获取类别概率。"
      ],
      "metadata": {
        "id": "TCsops2adey1"
      },
      "id": "TCsops2adey1"
    },
    {
      "cell_type": "markdown",
      "id": "19858180-0f26-43a8-b2c3-7ed40abf9f85",
      "metadata": {
        "id": "19858180-0f26-43a8-b2c3-7ed40abf9f85"
      },
      "source": [
        "## A.6 设置高效的数据加载器"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f98d8fc-5618-47a2-bc72-153818972a24",
      "metadata": {
        "id": "0f98d8fc-5618-47a2-bc72-153818972a24"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A6/1.png\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9dc2745-8be8-4344-80ef-325f02cda7b7",
      "metadata": {
        "id": "b9dc2745-8be8-4344-80ef-325f02cda7b7"
      },
      "outputs": [],
      "source": [
        "#首先创建一个简单的示例数据集，其中包含 5 个训练示例，每个示例有两个特征\n",
        "X_train = torch.tensor([\n",
        "    [-1.2, 3.1],\n",
        "    [-0.9, 2.9],\n",
        "    [-0.5, 2.6],\n",
        "    [2.3, -1.1],\n",
        "    [2.7, -1.5]\n",
        "])\n",
        "'''\n",
        "PyTorch 要求类别标签从标签 0 开始，并且最大的类别标签值不得超过输出节点数减 1\n",
        "（因为 Python 的索引从 0 开始）。因此，如果我们有类别标签 0、1、2、3 和 4，那么神经\n",
        "网络的输出层应包含 5 个节点。\n",
        "'''\n",
        "#与训练示例一起，我们还创建了一个包含相应类别标签的张量：3 个示例属于类别标签 0，两个示例属于类别标签 1\n",
        "y_train = torch.tensor([0, 0, 0, 1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88283948-5fca-461a-98a1-788b6be191d5",
      "metadata": {
        "id": "88283948-5fca-461a-98a1-788b6be191d5"
      },
      "outputs": [],
      "source": [
        "#还构建了一个包含两个样本的测试集\n",
        "X_test = torch.tensor([\n",
        "    [-0.8, 2.8],\n",
        "    [2.6, -1.6],\n",
        "])\n",
        "\n",
        "y_test = torch.tensor([0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edf323e2-1789-41a0-8e44-f3cab16e5f5d",
      "metadata": {
        "id": "edf323e2-1789-41a0-8e44-f3cab16e5f5d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset #通过继承 PyTorch 的 Dataset 父类来创建一个自定义数据集类 ToyDataset\n",
        "\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "    def __getitem__(self, index): #检索一条数据记录及其对应标签的说明\n",
        "        one_x = self.features[index]\n",
        "        one_y = self.labels[index]\n",
        "        return one_x, one_y\n",
        "\n",
        "    def __len__(self): #检索一条数据记录及其对应标签的说明\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "train_ds = ToyDataset(X_train, y_train)\n",
        "test_ds = ToyDataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上述代码片段定义了一个自定义的数据集类 `ToyDataset`，它继承自 PyTorch 的 `torch.utils.data.Dataset`。通过继承 `Dataset`，您的类可以使用 PyTorch 内置的数据处理功能，并能与 `DataLoader` 很好地集成。\n",
        "\n",
        "以下是该类及其方法的详细解释：\n",
        "\n",
        "1.  **`class ToyDataset(Dataset):`**: 这行代码定义了一个名为 `ToyDataset` 的新类，它继承自 `torch.utils.data.Dataset`。继承 `Dataset` 类可以让您的自定义数据集具备 PyTorch 数据集的基本功能。\n",
        "2.  **`def __init__(self, X, y):`**: 这是 `ToyDataset` 类的构造函数。它接收两个参数：\n",
        "    *   `X`: 预计是包含您的特征（输入数据）的张量或其他数据结构。\n",
        "    *   `y`: 预计是包含您的标签（目标输出）的张量或其他数据结构。\n",
        "    *   在构造函数内部，这些输入的 `X` 和 `y` 被存储为实例变量 `self.features` 和 `self.labels`。\n",
        "3.  **`def __getitem__(self, index):`**: 这是一个特殊的方法，它允许您使用索引（例如 `dataset[0]`）来访问数据集中的单个样本。它接收一个 `index` 作为输入。\n",
        "    *   `one_x = self.features[index]`: 这行代码检索给定 `index` 处样本的特征数据。\n",
        "    *   `one_y = self.labels[index]`: 这行代码检索给定 `index` 处样本的标签数据。\n",
        "    *   `return one_x, one_y`: 该方法返回一个元组，其中包含请求索引处的特征和标签。`DataLoader` 就是通过这个方法从您的数据集中检索单个样本的。\n",
        "4.  **`def __len__(self):`**: 这是另一个特殊的方法，它允许您使用 `len()` 函数（例如 `len(dataset)`）来获取数据集中的样本总数。\n",
        "    *   `return self.labels.shape[0]`: 这行代码返回标签张量（`self.labels.shape[0]`）的第一个维度的大小，这对应于数据集中的样本总数。\n",
        "\n",
        "在定义了类之后，接下来的两行代码：\n",
        "*   **`train_ds = ToyDataset(X_train, y_train)`**: 使用训练数据 `X_train` 和 `y_train` 创建了一个 `ToyDataset` 的实例，并将其赋值给 `train_ds` 变量。\n",
        "*   **`test_ds = ToyDataset(X_test, y_test)`**: 使用测试数据 `X_test` 和 `y_test` 创建了一个 `ToyDataset` 的实例，并将其赋值给 `test_ds` 变量。\n",
        "\n",
        "现在，`train_ds` 和 `test_ds` 对象就是 PyTorch 的 `Dataset` 对象，可以与 `DataLoader` 一起使用，以便在训练和评估期间高效地批量加载数据。"
      ],
      "metadata": {
        "id": "qb0A9XZcS_-r"
      },
      "id": "qb0A9XZcS_-r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7014705-1fdc-4f72-b892-d8db8bebc331",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7014705-1fdc-4f72-b892-d8db8bebc331",
        "outputId": "63505fb5-4fdc-4dd1-a352-e434e45d475f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ec6627a-4c3f-481a-b794-d2131be95eaf",
      "metadata": {
        "id": "3ec6627a-4c3f-481a-b794-d2131be95eaf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上述代码片段使用 `torch.utils.data.DataLoader` 为之前创建的 `train_ds` 数据集构建了一个数据加载器。数据加载器是 PyTorch 中用于高效加载和批量处理数据的重要工具。\n",
        "\n",
        "以下是代码的详细解释：\n",
        "\n",
        "1.  **`from torch.utils.data import DataLoader`**: 导入 PyTorch 中用于创建数据加载器的 `DataLoader` 类。\n",
        "2.  **`torch.manual_seed(123)`**: 设置 PyTorch 随机数生成器的种子。这确保了数据加载器在进行随机操作（例如打乱数据）时，结果是可重复的。\n",
        "3.  **`train_loader = DataLoader(...)`**: 创建一个 `DataLoader` 实例，并将其赋值给 `train_loader` 变量。\n",
        "    *   **`dataset=train_ds`**: 指定要加载数据的数据集对象，这里使用的是之前创建的 `train_ds`。\n",
        "    *   **`batch_size=2`**: 设置每个批次的数据大小为 2。这意味着数据加载器每次迭代将返回 2 个训练样本及其对应的标签。\n",
        "    *   **`shuffle=True`**: 设置为 `True` 表示在每个 epoch（训练周期）开始时，数据加载器会打乱数据集中的数据顺序。这有助于防止模型学习到数据中的顺序模式，提高模型的泛化能力。\n",
        "    *   **`num_workers=0`**: 设置用于数据加载的子进程数。`0` 表示数据加载将在主进程中进行。对于小型数据集或在调试时，设置为 `0` 通常足够。对于大型数据集，可以增加 `num_workers` 以利用多核处理器加速数据加载。\n",
        "\n",
        "创建 `train_loader` 后，您就可以在训练循环中迭代它，每次迭代都会为您提供一个包含指定批次大小的数据和标签的张量。"
      ],
      "metadata": {
        "id": "VzERYdyZSYeU"
      },
      "id": "VzERYdyZSYeU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9446de-5e4b-44fa-bf9a-a63e2661027e",
      "metadata": {
        "id": "8c9446de-5e4b-44fa-bf9a-a63e2661027e"
      },
      "outputs": [],
      "source": [
        "test_ds = ToyDataset(X_test, y_test)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "根据下面的输出，可以看到 train_loader 迭代了训练数据集，每个训练示例正好访问一次。\n",
        "这被称为一个训练轮次。由于我们使用 torch.manual_seed(123)设置了随机数生成器，因此\n",
        "你应该得到完全相同的训练示例打乱顺序。然而，当你再次迭代数据集时，你会发现打乱的顺序\n",
        "已经发生变化。这是为了防止深度神经网络在训练过程中陷入重复更新循环"
      ],
      "metadata": {
        "id": "B4PE3ezsUxA8"
      },
      "id": "B4PE3ezsUxA8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d4404c-9884-419f-979c-f659742d86ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99d4404c-9884-419f-979c-f659742d86ef",
        "outputId": "149ac357-422a-40f5-96e4-0cb2b89d88ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: tensor([[ 2.3000, -1.1000],\n",
            "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
            "Batch 2: tensor([[-1.2000,  3.1000],\n",
            "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
            "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
          ]
        }
      ],
      "source": [
        "#在实例化训练数据加载器后，可以对其进行迭代。对 test_loader 的迭代与之类似，但为简洁起见，这里省略了具体细节：\n",
        "for idx, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {idx+1}:\", x, y)\n",
        "#在这里指定的批次大小为 2，但第三批次仅包含一个示例。这是因为我们有 5 个训练示例，而 5 不能被 2 整除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d003f7e-7a80-40bf-a7fb-7a0d7dbba9db",
      "metadata": {
        "id": "9d003f7e-7a80-40bf-a7fb-7a0d7dbba9db"
      },
      "outputs": [],
      "source": [
        "#在实践中，如果一个训练轮次的最后一个批次显著小于其他批次，那么可能会影响训练过程中的收敛。为此，可以设置 drop_last=True，这将在每轮中丢弃最后一个批次\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    drop_last=True #drop_last=True，这将在每轮中丢弃最后一个批次\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db4d7f4-82da-44a4-b94e-ee04665d9c3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db4d7f4-82da-44a4-b94e-ee04665d9c3c",
        "outputId": "b8d82e50-219f-4495-c2ab-53f9ce83ca2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: tensor([[-1.2000,  3.1000],\n",
            "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
            "Batch 2: tensor([[ 2.3000, -1.1000],\n",
            "        [-0.9000,  2.9000]]) tensor([1, 0])\n"
          ]
        }
      ],
      "source": [
        "for idx, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {idx+1}:\", x, y) #可以看到最后一个批次被省略了"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataLoader 中的 num_workers=0 设置**:\n",
        "\n",
        "这个参数在 PyTorch 的DataLoader 函数中对于并行加载和预处理数据至关重要。当 num_workers 设置为 0 时，数据加载将在主进程而不是单独的工作进程中进行。这看起来似乎没有问题，但在使用** GPU 训练较大的网络**时，这可能会**导致模型训练显著减慢**。这是因为 CPU **不仅要处理深度学习模型**，还要**花时间加载和预处理数据**。因此，GPU 在等待 CPU 完成这些任务时可能会闲置。相反，当**num_workers 设置为大于 0** 的数值时，会**启动多个工作进程**并行加载数据，从而**释放主进程专注于训练模型**\n"
      ],
      "metadata": {
        "id": "Ie4Ac8UNVMs0"
      },
      "id": "Ie4Ac8UNVMs0"
    },
    {
      "cell_type": "markdown",
      "id": "eb03ed57-df38-4ee0-a553-0863450df39b",
      "metadata": {
        "id": "eb03ed57-df38-4ee0-a553-0863450df39b"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/MLNLP-World/LLMs-from-scratch-CN/main/imgs/A6/2.png\" width=\"600px\">\n",
        "\n",
        "在没有多个工作进程的情况下加载数据（设置 num_workers=0）会导致数据加载瓶\n",
        "颈，模型会在下一个批次加载完成前处于空闲状态（左）。如果启用多个工作进程，那\n",
        "么数据加载器可以在后台排队下一个批次的数据（右）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d904ca82-e50f-4f3d-a3ac-fc6ca53dd00e",
      "metadata": {
        "id": "d904ca82-e50f-4f3d-a3ac-fc6ca53dd00e"
      },
      "source": [
        "## A.7 典型的训练循环"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93f1791a-d887-4fc5-a307-5e5bde9e06f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93f1791a-d887-4fc5-a307-5e5bde9e06f6",
        "outputId": "40c2c9e4-90e6-48bb-c572-0105bc28741c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\n",
            "Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\n",
            "Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\n",
            "Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\n",
            "Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\n",
            "Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(num_inputs=2, num_outputs=2) # 该数据集有两个特征和两个类别\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5) # 优化器需要知道哪些参数需要优化\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "\n",
        "        logits = model(features)\n",
        "\n",
        "        loss = F.cross_entropy(logits, labels) # 损失函数\n",
        "\n",
        "        optimizer.zero_grad() #将上一轮的梯度置 0，以防止意外的梯度累积\n",
        "        loss.backward() #根据模型参数计算损失的梯度\n",
        "\n",
        "        optimizer.step() #优化器使用梯度更新模型参数\n",
        "\n",
        "        ### 日志\n",
        "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
        "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
        "              f\" | Train/Val Loss: {loss:.2f}\")\n",
        "\n",
        "    model.eval()\n",
        "    # 插入可选的模型评估代码"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上述代码片段实现了一个典型的 PyTorch 模型训练循环。它涵盖了神经网络训练的核心步骤，包括前向传播、计算损失、反向传播和参数更新。\n",
        "\n",
        "以下是代码的详细解释：\n",
        "\n",
        "1.  **`import torch.nn.functional as F`**: 导入 PyTorch 中的函数式接口，通常包含损失函数、激活函数等。这里主要用于使用 `F.cross_entropy`。\n",
        "2.  **`torch.manual_seed(123)`**: 设置 PyTorch 随机数生成器的种子，确保模型初始化和数据加载（如果 `DataLoader` 使用随机性）的可重复性。\n",
        "3.  **`model = NeuralNetwork(num_inputs=2, num_outputs=2)`**: 实例化之前定义的 `NeuralNetwork` 模型。这里输入特征数为 2，输出类别数为 2。\n",
        "4.  **`optimizer = torch.optim.SGD(model.parameters(), lr=0.5)`**: 创建一个随机梯度下降（SGD）优化器。\n",
        "    *   `model.parameters()`: 这会获取模型中所有需要梯度的参数（即需要训练的权重和偏置）。\n",
        "    *   `lr=0.5`: 设置学习率（learning rate）为 0.5，控制每次参数更新的步长。\n",
        "5.  **`num_epochs = 3`**: 设置训练的总轮次（epochs）为 3。一个 epoch 表示模型遍历整个训练数据集一次。\n",
        "6.  **`for epoch in range(num_epochs):`**: 外层循环，控制训练的 epoch 数量。\n",
        "7.  **`model.train()`**: 将模型设置为训练模式。这会启用 dropout 和 batch normalization 等在训练和评估阶段行为不同的层。\n",
        "8.  **`for batch_idx, (features, labels) in enumerate(train_loader):`**: 内层循环，迭代 `train_loader` 获取每个批次的数据和标签。\n",
        "    *   `enumerate(train_loader)`: 同时获取批次的索引 (`batch_idx`) 和数据 (`features`, `labels`)。\n",
        "    *   `(features, labels)`: `DataLoader` 每次迭代返回一个批次的特征和对应的标签。\n",
        "9.  **`logits = model(features)`**: **前向传播**。将当前批次的特征 `features` 输入到模型中，获取模型的原始输出（logits）。\n",
        "10. **`loss = F.cross_entropy(logits, labels)`**: **计算损失**。使用交叉熵损失函数计算模型输出 `logits` 与真实标签 `labels` 之间的损失。`F.cross_entropy` 内部已经集成了 Softmax 操作。\n",
        "11. **`optimizer.zero_grad()`**: **梯度清零**。在计算当前批次的梯度之前，将之前计算的梯度清零。这是必要的，因为 PyTorch 默认会将梯度累加。\n",
        "12. **`loss.backward()`**: **反向传播**。计算损失关于模型参数的梯度。\n",
        "13. **`optimizer.step()`**: **参数更新**。根据计算出的梯度和优化器的设置（例如学习率），更新模型的参数。\n",
        "14. **`### 日志`**: 代码中的注释，表示接下来的代码用于打印训练过程中的信息。\n",
        "15. **`print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\" ...)`**: 打印当前 epoch、批次信息以及训练损失。\n",
        "16. **`model.eval()`**: 将模型设置为评估模式。这会禁用 dropout 并将 batch normalization 层设置为使用整个数据集的均值和方差。在计算验证集或测试集的指标之前，务必设置为评估模式。\n",
        "17. **`# 插入可选的模型评估代码`**: 注释，提示这里可以插入计算模型在验证集上的性能（例如准确率）的代码。\n",
        "\n",
        "这个训练循环是大多数 PyTorch 项目的基础框架。通过重复执行前向传播、损失计算、反向传播和参数更新，模型逐渐学习从输入到输出的映射。"
      ],
      "metadata": {
        "id": "3DcuHXuUbxEr"
      },
      "id": "3DcuHXuUbxEr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**练习3:** 上述代码中介绍的神经网络有多少个参数？"
      ],
      "metadata": {
        "id": "cIDNOBWegVZb"
      },
      "id": "cIDNOBWegVZb"
    },
    {
      "cell_type": "code",
      "source": [
        "# here use model = NeuralNetwork(num_inputs=2, num_outputs=2) # 该数据集有两个特征和两个类别\n",
        "# then back to previous NeuralNetwork\n",
        "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of trainable model parameters:\", num_params)"
      ],
      "metadata": {
        "id": "vdxrsiM2gfFc"
      },
      "id": "vdxrsiM2gfFc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "在实际操作中，通常会使用第三个数据集，即所谓“验证数据集”，来找到最优的超参数设置。验证集类似于测试集。然而，虽然只想精确地使用一次测试集以避免评估偏差，但通常会多次使用验证集来调整模型设置。"
      ],
      "metadata": {
        "id": "iuZOEMO9k6zK"
      },
      "id": "iuZOEMO9k6zK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00dcf57f-6a7e-4af7-aa5a-df2cb0866fa5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00dcf57f-6a7e-4af7-aa5a-df2cb0866fa5",
        "outputId": "7fa7cdd0-0c0f-4bfc-83ab-4973709c3203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.8569, -4.1618],\n",
            "        [ 2.5382, -3.7548],\n",
            "        [ 2.0944, -3.1820],\n",
            "        [-1.4814,  1.4816],\n",
            "        [-1.7176,  1.7342]])\n"
          ]
        }
      ],
      "source": [
        "# 在训练好模型后，可以使用它进行预测\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_train)\n",
        "\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19be7390-18b8-43f9-9841-d7fb1919f6fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19be7390-18b8-43f9-9841-d7fb1919f6fd",
        "outputId": "7d6bc5b7-745a-4110-bd77-3532ffa27d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0.9991,     0.0009],\n",
            "        [    0.9982,     0.0018],\n",
            "        [    0.9949,     0.0051],\n",
            "        [    0.0491,     0.9509],\n",
            "        [    0.0307,     0.9693]])\n",
            "tensor([0, 0, 0, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# 为了获得类别成员概率，可以使用 PyTorch 的 softmax 函数\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "probas = torch.softmax(outputs, dim=1)\n",
        "print(probas)  #第一个值（列）表示该训练示例属于类别标签 0 的概率为 99.91%，属于类别标签 1 的概率为 0.09%。（这里使用 set_printoptions 是为了让输出更加易读。）\n",
        "\n",
        "predictions = torch.argmax(probas, dim=1) #This function returns the index of the maximum value along a specified dimension.\n",
        "#argmax 函数将这些概率值转换为类别标签预测。如果设置 dim=1，它将返回每行中最大值的索引位置（设置 dim=0 则返回每列中最大值的索引位置）\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07e7e530-f8d3-429c-9f5e-cf8078078c0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07e7e530-f8d3-429c-9f5e-cf8078078c0e",
        "outputId": "1e3a3d9a-fd56-47b9-a37a-8ab02a32969b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# 请注意，为了获得类别标签，计算 softmax 概率并非必需步骤，也可以直接对 logits（输出）应用argmax 函数\n",
        "'''\n",
        "为什么可以直接对 logits 使用 argmax？\n",
        "因为 softmax 函数是一个单调递增函数。这意味着如果一个类别在原始 logits 中的得分最高，\n",
        "那么它在经过 softmax 转换后的概率也一定是最高的。\n",
        "因此，为了找到概率最高的类别，我们只需要找到 logits 中得分最高的类别对应的索引即可，\n",
        "无需额外的 softmax 计算。这在计算上更有效率。\n",
        "'''\n",
        "predictions = torch.argmax(outputs, dim=1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f756f0d-63c8-41b5-a5d8-01baa847e026",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f756f0d-63c8-41b5-a5d8-01baa847e026",
        "outputId": "0eec1371-0171-4fac-d4de-ab20bf7190fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "predictions == y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da274bb0-f11c-4c81-a880-7a031fbf2943",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da274bb0-f11c-4c81-a880-7a031fbf2943",
        "outputId": "747f9b35-3f84-4d62-f94d-c8b3e54a9b10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "torch.sum(predictions == y_train) #使用 torch.sum 可以计算正确预测的数量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d62314-8dee-45b0-8f55-9e5aae2b24f4",
      "metadata": {
        "id": "16d62314-8dee-45b0-8f55-9e5aae2b24f4"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(model, dataloader):\n",
        "  '''\n",
        "  用于计算神经网络模型在给定数据集上的准确率\n",
        "  model: 要评估的神经网络模型。\n",
        "  dataloader: 用于提供数据的 DataLoader 对象（例如测试集或验证集的 DataLoader）。\n",
        "  '''\n",
        "    # 这行将模型设置为评估模式。在评估模式下，像 dropout 和 batch normalization 这样的层会表现得与训练模式不同（例如，dropout 会被禁用，batch normalization 会使用整个数据集的统计信息）。在计算准确率或进行推理时，务必将模型设置为评估模式\n",
        "    model = model.eval()\n",
        "    correct = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    for idx, (features, labels) in enumerate(dataloader):\n",
        "\n",
        "        with torch.no_grad(): #这是一个上下文管理器，用于指示 PyTorch 在此代码块内部的操作中不要跟踪梯度。在评估或推理阶段，我们不需要计算梯度，使用 torch.no_grad() 可以节省内存和计算资源。\n",
        "            logits = model(features) #将当前批次的特征 features 输入到模型中进行前向传播，得到模型的原始输出（logits）\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        compare = labels == predictions\n",
        "        correct += torch.sum(compare)\n",
        "        total_examples += len(compare)\n",
        "\n",
        "    return (correct / total_examples).item() #正确预测的比例是一个介于 0 和 1 之间的值。调用.item()会将张量的值以 Python 浮点数的形式返回\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6c9c17-2a5f-46c0-804b-873f169b729a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f6c9c17-2a5f-46c0-804b-873f169b729a",
        "outputId": "9f15d828-7e89-4837-9b82-484781455b39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "compute_accuracy(model, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311ed864-e21e-4aac-97c7-c6086caef27a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "311ed864-e21e-4aac-97c7-c6086caef27a",
        "outputId": "b6702ab9-9d80-43aa-f7c3-3b76d8d50de8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "compute_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d5cd469-3a45-4394-944b-3ce543f41dac",
      "metadata": {
        "id": "4d5cd469-3a45-4394-944b-3ce543f41dac"
      },
      "source": [
        "## A.8 保存和加载模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b013127d-a2c3-4b04-9fb3-a6a7c88d83c5",
      "metadata": {
        "id": "b013127d-a2c3-4b04-9fb3-a6a7c88d83c5"
      },
      "outputs": [],
      "source": [
        "# 下面是在PyTorch 中保存和加载模型的推荐方法\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "# 模型的 state_dict 是一个 Python 字典对象，它可以将模型中的每一层映射到其可训练参数（权重和偏置）\n",
        "# model.pth 是保存到磁盘的模型文件的任意文件名，我们可以使用任何名称和文件后缀，不过.pth 和.pt 是最常见的约定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b428c2-3a44-4d91-97c4-8298cf2b51eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2b428c2-3a44-4d91-97c4-8298cf2b51eb",
        "outputId": "2f46d011-550f-4c17-83eb-0c88aa9b9509"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# 保存模型后，可以从磁盘中恢复它\n",
        "model = NeuralNetwork(2, 2) # 需要与最初保存的模型完全匹配\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
        "# torch.load(\"model.pth\")函数读取文件 model.pth，并重建包含模型参数的 Python 字典对象\n",
        "# model.load_state_dict()则将这些参数应用到模型中，有效地恢复了我们保存模型时模型的学习状态"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**为什么在同一会话中 `model = NeuralNetwork(2, 2)` 不是严格必需的？**\n",
        "\n",
        "在您执行以下代码时：\n",
        "\n",
        "```python\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "```\n",
        "\n",
        "`model` 变量已经指向了您之前创建和训练好的那个 `NeuralNetwork(2, 2)` 模型的实例。\n",
        "\n",
        "如果您在**同一个 Python 会话中**紧接着执行：\n",
        "\n",
        "```python\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
        "```\n",
        "\n",
        "并且在保存模型后**没有对 `model` 变量进行重新赋值或改变其指向的对象**，那么 `model` 仍然是那个已经存在的模型实例。\n",
        "\n",
        "在这种情况下，**直接调用 `model.load_state_dict()` 就可以将从文件中加载的参数应用到这个现有的模型实例上**。因此，**再次创建 `model = NeuralNetwork(2, 2)` 并非强制性的。**\n",
        "\n",
        "---\n",
        "\n",
        "**为什么注释中包含了 `model = NeuralNetwork(2, 2)`，并且强调架构必须匹配？**\n",
        "\n",
        "为了说明需要模型实例：\n",
        "\n",
        "注释解释了为什么要包含：\n",
        "\n",
        "```python\n",
        "model = NeuralNetwork(2, 2)\n",
        "```\n",
        "\n",
        "即使在同一会话中不是必需的，它仍然是为了演示一个**通用的、跨会话的加载模型的流程**。\n",
        "\n",
        "当您在一个**新的 Python 会话或程序中加载模型**时，您不可能直接调用 `model.load_state_dict()`，因为此时内存中还没有一个名为 `model` 的神经网络实例。\n",
        "\n",
        "您必须先**创建一个与您保存时完全相同架构的模型实例**，然后才能将保存的参数加载到这个新的实例中。\n",
        "\n",
        "---\n",
        "\n",
        "强调架构匹配：\n",
        "\n",
        "`model.load_state_dict()` 方法是将一个字典（包含参数名称和对应的张量值）加载到一个模型实例中。\n",
        "\n",
        "这个过程是**基于参数名称和形状进行的**。如果新创建的模型实例的架构与保存时的模型不匹配（例如，层数不同、每层的节点数不同），那么参数的名称或形状就会不匹配，`load_state_dict` 方法就会报错，无法成功加载参数。\n",
        "\n",
        "因此，**确保加载模型的架构与保存模型的架构完全一致是至关重要的。**\n",
        "\n",
        "---\n",
        "\n",
        "总结：\n",
        "\n",
        "这段注释的目的是为了向您展示一个**通用的模型加载过程**，即使在某些特定情况下（如在同一会话中紧接着加载），**创建新的模型实例那一步可以省略**。\n",
        "\n",
        "但更重要的是强调：\n",
        "\n",
        "> **无论何时加载模型，都必须先在内存中创建一个与保存时架构完全一致的模型实例，然后才能使用 `load_state_dict` 方法将保存的参数加载到这个实例中。**"
      ],
      "metadata": {
        "id": "mfZdukWVpr1w"
      },
      "id": "mfZdukWVpr1w"
    },
    {
      "cell_type": "markdown",
      "id": "f891c013-43da-4a05-973d-997be313d2d8",
      "metadata": {
        "id": "f891c013-43da-4a05-973d-997be313d2d8"
      },
      "source": [
        "## A.9 使用 GPU 优化训练性能"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68ae888-cabf-49c9-bad6-ecdce774db57",
      "metadata": {
        "id": "e68ae888-cabf-49c9-bad6-ecdce774db57"
      },
      "source": [
        "### A.9.1 在GPU设备运行PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "141c845f-efe3-4614-b376-b8b7a9a2c887",
      "metadata": {
        "id": "141c845f-efe3-4614-b376-b8b7a9a2c887"
      },
      "source": [
        "见 [code-part2.ipynb](code-part2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99811829-b817-42ea-b03e-d35374debcc0",
      "metadata": {
        "id": "99811829-b817-42ea-b03e-d35374debcc0"
      },
      "source": [
        "### A.9.2 单个GPU训练"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b21456c-4af7-440f-9e78-37770277b5bc",
      "metadata": {
        "id": "0b21456c-4af7-440f-9e78-37770277b5bc"
      },
      "source": [
        "见 [code-part2.ipynb](code-part2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6eb2d1-a341-4489-b04b-635c26945333",
      "metadata": {
        "id": "db6eb2d1-a341-4489-b04b-635c26945333"
      },
      "source": [
        "### A.9.3 使用多个GPU训练"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d049a81-5fb0-49b5-9d6a-17a9976d8520",
      "metadata": {
        "id": "9d049a81-5fb0-49b5-9d6a-17a9976d8520"
      },
      "source": [
        "见 [DDP-script.py](DDP-script.py)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}