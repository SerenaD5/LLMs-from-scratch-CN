{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd",
      "metadata": {
        "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "<br>汉化的库: <a href=\"https://github.com/GoatCsu/CN-LLMs-from-scratch.git\">https://github.com/GoatCsu/CN-LLMs-from-scratch.git</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25aa40e3-5109-433f-9153-f5770531fe94",
      "metadata": {
        "id": "25aa40e3-5109-433f-9153-f5770531fe94"
      },
      "source": [
        "# 第二章:处理文本数据"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "原始仓库里ch02的README信息还有一个05 和一个video:\n",
        "\n",
        "- 05_bpe-from-scratch 包含（额外）代码，用于从头实现和训练 GPT-2 BPE 分词器。\n",
        "\n",
        "- 在下面的视频中，我提供了一个代码练习环节，其中涵盖了部分章节内容作为补充材料。\n",
        "https://www.youtube.com/watch?v=341Rb8fJxY0\n"
      ],
      "metadata": {
        "id": "94qxUsW5pfF6"
      },
      "id": "94qxUsW5pfF6"
    },
    {
      "cell_type": "markdown",
      "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf",
      "metadata": {
        "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf"
      },
      "source": [
        "本章节需要安装的包"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "778a1d01-7ca9-44a5-b8ce-00624232ac0b",
      "metadata": {
        "id": "778a1d01-7ca9-44a5-b8ce-00624232ac0b"
      },
      "source": [
        "pip3 install importlib.metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
      "metadata": {
        "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
        "outputId": "a07d5e11-ac7a-4af6-bb72-ee5f737a1236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.8.0+cu126\n",
            "tiktoken version: 0.11.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
        "# 确认库已安装并显示当前安装的版本"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a",
      "metadata": {
        "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a"
      },
      "source": [
        "- 本章节已经为LLM的实现构建了数据库"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5",
      "metadata": {
        "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2417139b-2357-44d2-bd67-23f5d7f52ae7",
      "metadata": {
        "id": "2417139b-2357-44d2-bd67-23f5d7f52ae7"
      },
      "source": [
        "## 2.1 理解文字embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6816ae-e927-43a9-b4dd-e47a9b0e1cf6",
      "metadata": {
        "id": "0b6816ae-e927-43a9-b4dd-e47a9b0e1cf6"
      },
      "source": [
        "- 无代码"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f69dab7-a433-427a-9e5b-b981062d6296",
      "metadata": {
        "id": "4f69dab7-a433-427a-9e5b-b981062d6296"
      },
      "source": [
        "- 在众多形式的embedding中,我们只讨论text embedding\n",
        "- embedding含义丰富,而且是常用词汇,所以以下皆不做翻译,多加体会!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba08d16f-f237-4166-bf89-0e9fe703e7b4",
      "metadata": {
        "id": "ba08d16f-f237-4166-bf89-0e9fe703e7b4"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "288c4faf-b93a-4616-9276-7a4aa4b5e9ba",
      "metadata": {
        "id": "288c4faf-b93a-4616-9276-7a4aa4b5e9ba"
      },
      "source": [
        "- LLM从高纬空间视角理解文字(i.e., 上千个dimension)\n",
        "- 虽然人类只能想象低维的视角,我们无法描绘计算机所理解的embedding\n",
        "- 但是下图我们粗浅的从二维上模拟计算机的视角"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b80160-1f10-4aad-a85e-9c79444de9e6",
      "metadata": {
        "id": "d6b80160-1f10-4aad-a85e-9c79444de9e6"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp\" width=\"300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
      "metadata": {
        "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3"
      },
      "source": [
        "## 2.2 文本标签化(tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4876035",
      "metadata": {
        "id": "e4876035"
      },
      "source": [
        "- 关于embedding token,实在是不好翻译,于是有时候我会选择不去翻译这两个专有名词\n",
        "- 本节中,我们将tokenize文字信息. 这会把文字拆解为更多小的理解单元 例如单个词或者字节"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747d73d0",
      "metadata": {
        "id": "747d73d0"
      },
      "source": [
        "(这也有点抽象，事实上可以粗略理解为将单词拆分为词根、词源和词缀)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b",
      "metadata": {
        "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp\" width=\"300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cceaa18-833d-46b6-b211-b20c53902805",
      "metadata": {
        "id": "8cceaa18-833d-46b6-b211-b20c53902805"
      },
      "source": [
        "- 载入源文件\n",
        "- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) 一本无版权的短篇小说"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "27e9b441-cf4e-4a4e-8e3e-44be25354259",
      "metadata": {
        "id": "27e9b441-cf4e-4a4e-8e3e-44be25354259"
      },
      "outputs": [],
      "source": [
        "import os##导入os库\n",
        "import urllib.request ##导入request库\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):##如果文件不存在则创建，防止因文件已存在而报错\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)##从指定的地点读取文件"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56488f2c-a2b8-49f1-aaeb-461faad08dce",
      "metadata": {
        "id": "56488f2c-a2b8-49f1-aaeb-461faad08dce"
      },
      "source": [
        "- 如果在执行前面的代码单元时遇到 `ssl.SSLCertVerificationError` 错误可能是由于使用了过时的 Python 版本；\n",
        "- 你可以在 [GitHub 上查阅更多信息](https://github.com/rasbt/LLMs-from-scratch/pull/403)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8a769e87-470a-48b9-8bdb-12841b416198",
      "metadata": {
        "id": "8a769e87-470a-48b9-8bdb-12841b416198",
        "outputId": "e98b852b-69ce-415a-c52c-7509a65dadb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read() ##读入文件按照utf-8\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))##先输出总长度\n",
        "print(raw_text[:99])##输出前一百个内容"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e",
      "metadata": {
        "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e"
      },
      "source": [
        "- 目标是对这段文本进行分词和嵌入处理，以便用于大语言模型（LLM）。\n",
        "- 我们将基于一些简单的示例文本开发一个简单的分词器，之后可以将其应用于上述文本。\n",
        "- 以下正则表达式将基于空格进行分割。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
      "metadata": {
        "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
        "outputId": "7640bfa9-807f-4b97-c240-f5b6f775fff8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)##正则表达式按照空白字符进行分割\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c40c18-a9d5-4703-bf71-8261dbcc5ee3",
      "metadata": {
        "id": "a8c40c18-a9d5-4703-bf71-8261dbcc5ee3"
      },
      "source": [
        "- 优化正则表达,可以分割更多的符号"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ea02489d-01f9-4247-b7dd-a0d63f62ef07",
      "metadata": {
        "id": "ea02489d-01f9-4247-b7dd-a0d63f62ef07",
        "outputId": "a7dd8dc1-0e9f-4752-d056-37f365cc3b56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(r'([,.]|\\s)', text)##只是按照, .分割\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461d0c86-e3af-4f87-8fae-594a9ca9b6ad",
      "metadata": {
        "id": "461d0c86-e3af-4f87-8fae-594a9ca9b6ad"
      },
      "source": [
        "- 移除空格"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4d8a6fb7-2e62-4a12-ad06-ccb04f25fed7",
      "metadata": {
        "id": "4d8a6fb7-2e62-4a12-ad06-ccb04f25fed7",
        "outputId": "b146637d-057c-4fb0-bba3-6fc38557eb4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "##把上述结果去掉空格\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250e8694-181e-496f-895d-7cb7d92b5562",
      "metadata": {
        "id": "250e8694-181e-496f-895d-7cb7d92b5562"
      },
      "source": [
        "- 我们还需要处理其他标点符号"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ed3a9467-04b4-49d9-96c5-b8042bcf8374",
      "metadata": {
        "id": "ed3a9467-04b4-49d9-96c5-b8042bcf8374",
        "outputId": "760a5439-a916-4a88-e6e8-0a382560e954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) ##就是按照常用的符号分割\n",
        "result = [item.strip() for item in result if item.strip()]##去掉两端的空白字符 也是去掉了空字符串与仅包含空白字符的项\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bbea70b-c030-45d9-b09d-4318164c0bb4",
      "metadata": {
        "id": "5bbea70b-c030-45d9-b09d-4318164c0bb4"
      },
      "source": [
        "- 万事俱备，我们现在来看一下文字处理后的效果"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a",
      "metadata": {
        "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/05.webp\" width=\"350px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8c567caa-8ff5-49a8-a5cc-d365b0a78a99",
      "metadata": {
        "id": "8c567caa-8ff5-49a8-a5cc-d365b0a78a99",
        "outputId": "230bbb8e-2cd2-4b82-e22c-bea28707b359",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text) ##按照符号继续把原文件给分割了\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]##去掉两端的空白字符 也是去掉了空字符串和仅包含空白字符的项\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a19e1a-5105-4ddb-812a-b7d3117eab95",
      "metadata": {
        "id": "e2a19e1a-5105-4ddb-812a-b7d3117eab95"
      },
      "source": [
        "- 查看token的长度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c",
      "metadata": {
        "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c",
        "outputId": "73d24530-2aaf-4fdd-a765-4a103ff1c7ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231",
      "metadata": {
        "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231"
      },
      "source": [
        "## 2.3 将词元转换为词元 ID (给token编号)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
      "metadata": {
        "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff"
      },
      "source": [
        "- 通过如下的embedding层,我们可以给token编号\n",
        "- 把这些词元从 Python 字符串转换为整数表示，以生成词元 ID（token ID）。这一过程是将词元 ID 转换为嵌入向量前的必经步骤"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d",
      "metadata": {
        "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5973794-7002-4202-8b12-0900cd779720",
      "metadata": {
        "id": "b5973794-7002-4202-8b12-0900cd779720"
      },
      "source": [
        "- 为了将先前生成的词元映射到词元 ID，首先需要构建一张词汇表。这张词汇表定义了如何将每个唯一的单词和特殊字符映射到一个唯一的整数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
      "metadata": {
        "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
        "outputId": "c86925ac-493f-460d-ae53-46545d0955a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ],
      "source": [
        "all_words = sorted(set(preprocessed))#从去掉重复的字符\n",
        "vocab_size = len(all_words)#计总的单词书\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "77d00d96-881f-4691-bb03-84fec2a75a26",
      "metadata": {
        "id": "77d00d96-881f-4691-bb03-84fec2a75a26"
      },
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}##先把word进行编号,再按照单词或者标点为索引(有HashList那味道了)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3",
      "metadata": {
        "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3"
      },
      "source": [
        "- 看一下前50个是怎样的"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
      "metadata": {
        "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
        "outputId": "9e6b138c-d8f4-4760-ef8f-f55aa6d9aedf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break ##遍历到前五十个"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19",
      "metadata": {
        "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19"
      },
      "source": [
        "- 接下来,我们将通过一个短文本来感受下,处理后的效果是怎样的"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc",
      "metadata": {
        "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a",
      "metadata": {
        "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a"
      },
      "source": [
        "- 现在将所有内容整合到一个分词器类中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5",
      "metadata": {
        "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:#一个实例的名字创立\n",
        "    def __init__(self, vocab): ## 初始化一个字符串\n",
        "        self.str_to_int = vocab #单词到整数的映射, 将词汇表作为类属性存储，以便在 encode方法和 decode 方法中访问\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()} #创建逆向词汇表，将词元 ID 映射回原始文本词元\n",
        "        #方便解码,进行整数到词汇的反向映射\n",
        "\n",
        "    def encode(self, text):\n",
        "        '''\n",
        "        处理输入文本，将其转换为词元 ID\n",
        "        '''\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)##正则化分词标点符号\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()## 去掉两端空格与全部的空句\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]##整理完的额字符串列表对应到id,从字典出来\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids): #将词元ID转换回文本\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids]) #映射整数id到字符串。join是用前面那个(“ ”)联结成一个完整的字符串\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #使用正则表达式，去除标点符号前的多余空格\n",
        "        # \\s+匹配一个或者多个空白  \\1 替换到匹配\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SimpleTokenizerV1` 的简单分词器类。这个类的主要作用是将文本转换为数字（词元 ID），以及将数字（词元 ID）转换回文本。\n",
        "\n",
        "以下是代码的详细解释：\n",
        "\n",
        "- **`class SimpleTokenizerV1:`**: 定义了一个名为 `SimpleTokenizerV1` 的类。\n",
        "- **`__init__(self, vocab)`**: 这是类的构造函数，在创建 `SimpleTokenizerV1` 对象时被调用。\n",
        "    - `self.str_to_int = vocab`: 将输入的 `vocab` 字典（用于将字符串词元映射到整数 ID）存储为类的属性 `str_to_int`。\n",
        "    - `self.int_to_str = {i:s for s,i in vocab.items()}`: 创建一个逆向字典 `int_to_str`，用于将整数 ID 映射回字符串词元。\n",
        "- **`encode(self, text)`**: 这个方法用于将输入的文本字符串编码为词元 ID 的列表。\n",
        "    - `preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)`: 使用正则表达式按照标点符号和空白字符分割输入的文本。\n",
        "    - `preprocessed = [item.strip() for item in preprocessed if item.strip()]`: 遍历分割后的结果，去除每个词元两端的空白字符，并过滤掉只包含空白字符的项，得到一个干净的词元列表。\n",
        "    - `ids = [self.str_to_int[s] for s in preprocessed]`: 遍历处理后的词元列表，使用 `str_to_int` 字典将每个词元映射到其对应的整数 ID，生成一个整数 ID 的列表。\n",
        "    - `return ids`: 返回生成的词元 ID 列表。\n",
        "- **`decode(self, ids)`**: 这个方法用于将输入的词元 ID 列表解码回文本字符串。\n",
        "    - `text = \" \".join([self.int_to_str[i] for i in ids])`: 遍历输入的词元 ID 列表，使用 `int_to_str` 字典将每个 ID 映射回其对应的字符串词元，然后使用空格将这些词元连接成一个字符串。\n",
        "    - `text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)`: 使用正则表达式去除标点符号前多余的空格，使解码后的文本更自然。`\\s+` 匹配一个或多个空白字符，`([,.?!\"()\\'])` 捕获一个标点符号，`\\1` 表示替换为捕获到的标点符号本身。\n",
        "    - `return text`: 返回解码后的文本字符串。\n",
        "\n",
        "总的来说，`SimpleTokenizerV1` 类提供了一个基本的分词和反分词功能，它依赖于一个预先构建好的词汇表来将词元和整数 ID 进行相互转换。"
      ],
      "metadata": {
        "id": "brR31_RxDVSz"
      },
      "id": "brR31_RxDVSz"
    },
    {
      "cell_type": "markdown",
      "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
      "metadata": {
        "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7"
      },
      "source": [
        "- `encode` 函数将文本转换为标记 ID。\n",
        "- `decode` 函数将标记 ID 转换回文本。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc21d347-ec03-4823-b3d4-9d686e495617",
      "metadata": {
        "id": "cc21d347-ec03-4823-b3d4-9d686e495617"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c",
      "metadata": {
        "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c"
      },
      "source": [
        "- 我们可以使用分词器将文本编码（即分词）为数字。\n",
        "- 然后，这些整数可以作为大语言模型（LLM）的输入，进行嵌入。\n",
        "- 分词器通常包含两个常见的方法：encode 方法和 decode 方法。encode 方法接收文本样本，将其分词为单独的词元，然后再利用词汇表将词元转换为词元 ID。而 decode 方法接收一组词元 ID，将其转换回文本词元，并将文本词元连接起来，形成自然语言文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
      "metadata": {
        "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
        "outputId": "6a69907a-586a-4f0d-9373-791778b1726b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab) #用vocab创造一个实例\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text) #按照这个例子里的encode函数处理text\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3201706e-a487-4b60-b99d-5765865f29a0",
      "metadata": {
        "id": "3201706e-a487-4b60-b99d-5765865f29a0"
      },
      "source": [
        "- 把数字重新映射回文字"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
      "metadata": {
        "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
        "outputId": "e30217ef-e905-4c16-9a65-5e4bcc02da18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "tokenizer.decode(ids)#按照这个例子里的decode函数处理text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "54f6aa8b-9827-412e-9035-e827296ab0fe",
      "metadata": {
        "id": "54f6aa8b-9827-412e-9035-e827296ab0fe",
        "outputId": "a67183e2-0f83-47ef-c6d7-4b4b102f55b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))#按照这个例子里的decode函数处理(#按照这个例子里的encode函数处理text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b821ef8-4d53-43b6-a2b2-aef808c343c7",
      "metadata": {
        "id": "4b821ef8-4d53-43b6-a2b2-aef808c343c7"
      },
      "source": [
        "## 2.4 添加特殊标记"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863d6d15-a3e2-44e0-b384-bb37f17cf443",
      "metadata": {
        "id": "863d6d15-a3e2-44e0-b384-bb37f17cf443"
      },
      "source": [
        "- 文本的结尾需要特别的符号来表明"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa7fc96c-e1fd-44fb-b7f5-229d7c7922a4",
      "metadata": {
        "id": "aa7fc96c-e1fd-44fb-b7f5-229d7c7922a4"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/09.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d709d57-2486-4152-b7f9-d3e4bd8634cd",
      "metadata": {
        "id": "9d709d57-2486-4152-b7f9-d3e4bd8634cd"
      },
      "source": [
        "- 一些分词器使用特殊标记来帮助大语言模型（LLM）获取额外的上下文信息。\n",
        "- 其中一些特殊标记包括：\n",
        "  - `[BOS]`（序列开始）表示文本的开始。\n",
        "  - `[EOS]`（序列结束）表示文本的结束（通常用于连接多个不相关的文本，例如两个不同的维基百科文章或两本不同的书籍等）。\n",
        "  - `[PAD]`（填充）如果我们使用大于1的批次大小训练LLM（我们可能会包含不同长度的多篇文本），使用填充标记将较短的文本填充至最长的长度，以确保所有文本具有相同的长度。\n",
        "- `[UNK]` 用于表示词汇表中没有的词。\n",
        "\n",
        "- 请注意，GPT-2不需要上述提到的这些标记，它只使用 `<|endoftext|>` 标记。\n",
        "- `<|endoftext|>` 类似于上述提到的 `[EOS]` 标记。\n",
        "- GPT 还使用 `<|endoftext|>` 进行填充（因为我们在批量输入训练时通常使用掩码，所以无论这些填充标记是什么，都不会影响模型的训练，因为填充标记不会被关注）。\n",
        "- GPT-2 不使用 `<UNK>` 标记来表示词汇表外的词；相反，GPT-2 使用字节对编码（BPE）分词器，将单词分解为子词单元，后续将进一步讨论这一点。\n",
        "- 我们在两个独立文本之间使用 `<|endoftext|>` 标记："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52442951-752c-4855-9752-b121a17fef55",
      "metadata": {
        "id": "52442951-752c-4855-9752-b121a17fef55"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c661a397-da06-4a86-ac27-072dbe7cb172",
      "metadata": {
        "id": "c661a397-da06-4a86-ac27-072dbe7cb172"
      },
      "source": [
        "- 看一下接下来会发生什么"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d5767eff-440c-4de1-9289-f789349d6b85",
      "metadata": {
        "id": "d5767eff-440c-4de1-9289-f789349d6b85"
      },
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)  ##用vocab创造一个实例\n",
        "\n",
        "text = \"Hello, do you like tea. Is this-- a test?\"\n",
        "\n",
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc53ee0c-fe2b-4cd8-a946-5471f7651acf",
      "metadata": {
        "id": "dc53ee0c-fe2b-4cd8-a946-5471f7651acf"
      },
      "source": [
        "- 上述操作会产生一个错误，因为单词“Hello”不在词汇表中。\n",
        "- 为了处理这种情况，我们可以向词汇表中添加类似 `\"<|unk|>\"` 的特殊标记，用于表示未知词汇。\n",
        "- 因为我们已经在扩展词汇表，那么我们可以再添加一个标记 `\"<|endoftext|>\"`，该标记在GPT-2训练中用于表示文本的结束（它也用于连接的文本之间，例如当我们的训练数据集包含多篇文章、书籍等时）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f",
      "metadata": {
        "id": "ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f"
      },
      "outputs": [],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))#set去重 list把处理后的重新变为列表,然后排序\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])#加上未知的表示\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "#遍历 enumerate(all_tokens) 中的每个元组 (integer, token)，以 token 作为键，integer 作为值创建字典条目。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "57c3143b-e860-4d3b-a22a-de22b547a6a9",
      "metadata": {
        "id": "57c3143b-e860-4d3b-a22a-de22b547a6a9",
        "outputId": "94d02049-69da-42bb-8c2d-a87a06478c52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "len(vocab.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "50e51bb1-ae05-4aa8-a9ff-455b65ed1959",
      "metadata": {
        "id": "50e51bb1-ae05-4aa8-a9ff-455b65ed1959",
        "outputId": "9d28caab-e992-4167-b11f-01ed88bb03d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):#输出后五个内容与其标号\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1daa2b0-6e75-412b-ab53-1f6fb7b4d453",
      "metadata": {
        "id": "a1daa2b0-6e75-412b-ab53-1f6fb7b4d453"
      },
      "source": [
        "- 因此增加 `<unk>`不失为一种好的选择"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "948861c5-3f30-4712-a234-725f20d26f68",
      "metadata": {
        "id": "948861c5-3f30-4712-a234-725f20d26f68"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV2:##版本2.0,启动!\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}#s为单词,i是key\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)#正则化按照标点分类\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]#去掉两头与所有空余句\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "            #遍历 preprocessed 中的每个 item，如果 item 存在于 self.str_to_int（即词汇表）中，就保留 item\n",
        "            #如果不存在（即该单词或符号未定义在词汇表中），就替换为特殊标记 <|unk|>。\n",
        "            #拓展:推导式（如列表推导式）是一种紧凑的语法，专门用于生成新列表（或其他容器）\n",
        "            #与普通 for 循环相比，它更加简洁和高效，但逻辑复杂时可能会降低可读性。\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]#单词或标点映射为整数列表\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b2e942",
      "metadata": {
        "id": "e9b2e942"
      },
      "source": [
        "- 用优化后的分词器对文本进行操作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4133c502-18ac-4412-9f43-01caf4efa3dc",
      "metadata": {
        "id": "4133c502-18ac-4412-9f43-01caf4efa3dc",
        "outputId": "f2074333-b557-4971-cc2b-2f56a27d8f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ],
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))#用句子分隔符链接两个句子\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "7ed395fe-dc1b-4ed2-b85b-457cc35aab60",
      "metadata": {
        "id": "7ed395fe-dc1b-4ed2-b85b-457cc35aab60",
        "outputId": "090ca929-8bae-4dc4-a2e2-289e8e4d03ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "tokenizer.encode(text)#跟第一个一样,但不会报错了"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "059367f9-7a60-4c0d-8a00-7c4c766d0ebc",
      "metadata": {
        "id": "059367f9-7a60-4c0d-8a00-7c4c766d0ebc",
        "outputId": "59763cd4-83f8-4f32-8c9b-127dcac40d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
      "metadata": {
        "id": "5c4ba34b-170f-4e71-939b-77aabb776f14"
      },
      "source": [
        "## 2.5 字节对编码"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
      "metadata": {
        "id": "2309494c-79cf-4a2d-bc28-a94d602f050e"
      },
      "source": [
        "- GPT-2 使用字节对编码（BPE）作为其分词器。\n",
        "- 这种方式允许模型将不在预定义词汇表中的单词分解为更小的子词单元，甚至是单个字符，从而使其能够处理词汇表外的词汇。\n",
        "- 例如，如果 GPT-2 的词汇表中没有“unfamiliarword”这个单词，它可能会将其分词为 [\"unfam\", \"iliar\", \"word\"]，或者根据训练好的 BPE 合并规则进行其他子词分解。\n",
        "- 原始的 BPE 分词器可以在这里找到：[https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
        "- 在本章中，我们使用了 OpenAI 开源的 [tiktoken](https://github.com/openai/tiktoken) 库中的 BPE 分词器，该库在 Rust 中实现了核心算法，以提高计算性能。\n",
        "- 在 [./bytepair_encoder](../02_bonus_bytepair-encoder) 中创建了一个笔记本，对比了这两种实现的效果（在样本文本上，tiktoken 的速度大约快了 5 倍）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "ede1d41f-934b-4bf4-8184-54394a257a94",
      "metadata": {
        "id": "ede1d41f-934b-4bf4-8184-54394a257a94"
      },
      "outputs": [],
      "source": [
        "# pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
      "metadata": {
        "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
        "outputId": "deef8023-87b3-4162-9fae-f728e35d23f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.11.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))#验证下载并输出版本信息"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
      "metadata": {
        "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")#初始化GPT2! 按照以下方式实例化 tiktoken 中的 BPE 分词器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5ff2cd85-7cfb-4325-b390-219938589428",
      "metadata": {
        "id": "5ff2cd85-7cfb-4325-b390-219938589428",
        "outputId": "71106372-db9e-4c16-e987-1b066a11e854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})#输出分词的id,可以允许endoftext\n",
        "\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. <|endoftext|>词元被分配了一个较大的词元 ID，即 50256。事实上，用于训练GPT-2、GPT-3 和 ChatGPT 中使用的原始模型的 BPE 分词器的词汇总量为 50 257，这意味着\n",
        "<|endoftext|>被分配了最大的词元 ID。\n",
        "2. BPE 分词器可以正确地编码和解码未知单词，比如“someunknownPlace”。BPE 分词器是如何做到在不使用<|unk|>词元的前提下处理任何未知词汇的呢？\n",
        "- BPE 算法的原理是将不在预定义词汇表中的单词分解为更小的子词单元甚至单个字符，从而能够处理词汇表之外的单词。因此，得益于 BPE 算法，如果分词器在分词过程中遇到不熟悉的单词，它可以将其表示为子词词元或字符序列"
      ],
      "metadata": {
        "id": "70dAR65sU3rO"
      },
      "id": "70dAR65sU3rO"
    },
    {
      "cell_type": "markdown",
      "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a",
      "metadata": {
        "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a"
      },
      "source": [
        "- BPE tokenizers将未知词汇分解为子词和单个字符。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c",
      "metadata": {
        "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"300px\">"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_testing = (\"Akwirwier\")\n",
        "\n",
        "integers_testing = tokenizer.encode(text_testing, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(\"词元ID: \",integers_testing)\n",
        "\n",
        "strings_testing = tokenizer.decode(integers_testing)\n",
        "\n",
        "print(\"词元: \",strings_testing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq6mcJ_8WfQQ",
        "outputId": "932d6a83-4500-4392-e26b-6fe7f3eafd75"
      },
      "id": "uq6mcJ_8WfQQ",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "词元ID:  [33901, 86, 343, 86, 959]\n",
            "词元:  Akwirwier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
      "metadata": {
        "id": "abbd7c0d-70f8-4386-a114-907e96c950b0"
      },
      "source": [
        "## 2.6 利用滑动窗口进行数据采样"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0",
      "metadata": {
        "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0"
      },
      "source": [
        "- 现在我们训练的大语言模型（LLMs）时是一次生成一个单词，因此希望根据训练数据的要求进行准备，使得序列中的下一个单词作为预测目标。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
      "metadata": {
        "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "实现一个数据加载器，使用**滑动窗口**（sliding window）方法从训练数据集中提取,**首先**，使用 BPE 分词器对短篇小说 The Verdict 的全文进行分词"
      ],
      "metadata": {
        "id": "QGSXHxcMZgJ4"
      },
      "id": "QGSXHxcMZgJ4"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "848d5ade-fd1f-46c3-9e31-1426e315c71b",
      "metadata": {
        "id": "848d5ade-fd1f-46c3-9e31-1426e315c71b",
        "outputId": "45de7ee2-8c7f-4092-ff30-84660ab836a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)#读入了一个text并编码到enc_text里面\n",
        "print(len(enc_text)) #是应用 BPE 分词器后训练集中的词元总数"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cebd0657-5543-43ca-8011-2ae6bd0a5810",
      "metadata": {
        "id": "cebd0657-5543-43ca-8011-2ae6bd0a5810"
      },
      "source": [
        "- 对于每个文本块，我们需要输入和目标。\n",
        "- 由于我们希望模型预测下一个单词，因此我们要生成目标是将输入右移一个位置后的单词。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "e84424a7-646d-45b6-99e3-80d15fb761f2",
      "metadata": {
        "id": "e84424a7-646d-45b6-99e3-80d15fb761f2"
      },
      "outputs": [],
      "source": [
        "enc_sample = enc_text[50:]#从第五十一个开始向后,从数据集中移除前 50 个词元以便演示"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "创建下一单词预测任务的输入-目标对的一种简单且直观的方法是定义两个变量：x 和 y。变量 x 用于存储输入的词元，变量 y 则用于存储由 x 的每个输入词元右移一个位置所得的目标词元"
      ],
      "metadata": {
        "id": "3wXT_r-AglVd"
      },
      "id": "3wXT_r-AglVd"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "dfbff852-a92f-48c8-a46d-143a0f109f40",
      "metadata": {
        "id": "dfbff852-a92f-48c8-a46d-143a0f109f40",
        "outputId": "f6d668e4-e633-4864-80f1-f13e93f7f94e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ],
      "source": [
        "context_size = 4 #sliding windows4\n",
        "\n",
        "x = enc_sample[:context_size]#开头四个\n",
        "y = enc_sample[1:context_size+1]#第二个开始的四个\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815014ef-62f7-4476-a6ad-66e20e42b7c3",
      "metadata": {
        "id": "815014ef-62f7-4476-a6ad-66e20e42b7c3"
      },
      "source": [
        "- 就像预言家一个晚上只能预言一个玩家,我们的模型一次也只能预测一个单词"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "d97b031e-ed55-409d-95f2-aeb38c6fe366",
      "metadata": {
        "id": "d97b031e-ed55-409d-95f2-aeb38c6fe366",
        "outputId": "daa9a399-1148-4789-9978-2219c608867f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    #文本成输入 context,先输出有什么,然后输出下一个是什么编号\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "f57bd746-dcbf-4433-8e24-ee213a8c34a1",
      "metadata": {
        "id": "f57bd746-dcbf-4433-8e24-ee213a8c34a1",
        "outputId": "130f4984-13be-44a4-9c3b-fd3c9e3824fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    #文本成输入 context,先输出有什么,然后输出下一个是什么单词\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210d2dd9-fc20-4927-8d3d-1466cf41aae1",
      "metadata": {
        "id": "210d2dd9-fc20-4927-8d3d-1466cf41aae1"
      },
      "source": [
        "- 我们将在后续章节中处理下一个单词预测，届时会介绍注意力机制。\n",
        "- 目前，我们实现一个简单的数据加载器，它遍历输入数据集并返回右移一个位置后的输入和目标。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1a1b47a-f646-49d1-bc70-fddf2c840796",
      "metadata": {
        "id": "a1a1b47a-f646-49d1-bc70-fddf2c840796"
      },
      "source": [
        "- 安装并导入 PyTorch（安装提示请参见附录A）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "e1770134-e7f3-4725-a679-e04c3be48cac",
      "metadata": {
        "id": "e1770134-e7f3-4725-a679-e04c3be48cac",
        "outputId": "a2df0af4-1bb5-45ff-c38b-27aae9f59a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
      "metadata": {
        "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c"
      },
      "source": [
        "- 用滑动窗口法运行,窗口位置每次加一\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c",
      "metadata": {
        "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c"
      },
      "source": [
        "- 创建数据集和数据加载器，从输入文本数据集中提取文本块。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "一个用于批处理输入和目标的数据集"
      ],
      "metadata": {
        "id": "qDjeeB2liZfp"
      },
      "id": "qDjeeB2liZfp"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375",
      "metadata": {
        "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "#Dataset 是 PyTorch 中用于表示数据集的抽象类，DataLoader 则用于高效地加载和批量处理数据\n",
        "\n",
        "class GPTDatasetV1(Dataset):#定义了一个继承自 Dataset 的新类 GPTDatasetV1\n",
        "    #让GPT初始化一个类型\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):# 这是类的构造函数，用于初始化数据集\n",
        "        self.input_ids = [] #初始化两个空列表，分别用于存储输入的词元 ID 序列\n",
        "        self.target_ids = [] #目标词元 ID 序列\n",
        "\n",
        "        # 对全部文本进行分词 -使用传入的 tokenizer 对输入的文本 txt 进行编码，将文本转换为词元 ID 列表\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})#id是文本内容编码过来的\n",
        "        #允许分词器处理 <|endoftext|> 特殊标记\n",
        "\n",
        "        # 使用滑动窗口将文本划分为长度为 max_length 的重叠序列\n",
        "        for i in range(0, len(token_ids) - max_length, stride): #使用滑动窗口的方式从 token_ids 中提取序列\n",
        "            #从 0 开始，到 len(token_ids) - max_length 结束（不包含），步长为 stride 的整数序列\n",
        "            input_chunk = token_ids[i:i + max_length] #提取从索引 i 开始，长度为 max_length 的词元 ID 序列作为输入\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1] #提取从索引 i + 1 开始，长度为 max_length 的词元 ID 序列作为目标。这实现了下一个词预测的任务，目标序列是输入序列向右移动一个位置的结果\n",
        "            self.input_ids.append(torch.tensor(input_chunk)) #将 input_chunk 转换为 PyTorch 张量并添加到 self.input_ids 列表中\n",
        "            self.target_ids.append(torch.tensor(target_chunk)) #将 target_chunk 转换为 PyTorch 张量并添加到 self.target_ids 列表中\n",
        "\n",
        "    def __len__(self): #这是 Dataset 类必须实现的方法，返回数据集中的样本数量, 即返回数据集的总行数\n",
        "        return len(self.input_ids) #在这里，它返回 self.input_ids 列表的长度，因为每个输入序列都有一个对应的目标序列\n",
        "\n",
        "    def __getitem__(self, idx): #这是 Dataset 类必须实现的方法，用于根据索引 idx 获取数据集中的一个样本, 即返回数据集的指定行\n",
        "        return self.input_ids[idx], self.target_ids[idx] #返回 self.input_ids 中索引为 idx 的输入序列和 self.target_ids 中索引为 idx 的目标序列\n",
        "\n",
        "#stride 参数控制了窗口的重叠程度，较大的 stride 会减少样本数量和重叠，较小的 stride 会增加样本数量和重叠"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "用于批量生成输入-目标对的数据加载器"
      ],
      "metadata": {
        "id": "dD3zYnYzlQ_F"
      },
      "id": "dD3zYnYzlQ_F"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e",
      "metadata": {
        "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # 初始化分词器 -初始化一个 GPT-2 的分词器，使用 tiktoken 库\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # 创建数据集\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #这个对象包含了处理后的输入和目标序列\n",
        "\n",
        "    # 创建 dataloader 对象\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last, #如果 drop_last 为 True 且批次大小小于指定的 batch_size，则会删除最后一批，以防止在训练期间出现损失剧增\n",
        "        num_workers=num_workers #用于预处理的 CPU 进程数\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**函数参数意思**:\n",
        "- txt: 输入的原始文本数据。\n",
        "- batch_size=4: 每个批次包含的样本数量，默认为 4。\n",
        "- max_length=256: 每个输入序列的最大长度，默认为 256。\n",
        "- stride=128: 滑动窗口的步长，默认为 128。\n",
        "- shuffle=True: 是否在每个 epoch 开始时打乱数据，默认为 True。\n",
        "- drop_last=True: 如果数据集大小不能被批次大小整除，是否丢弃最后一个不完整的批次，默认为 True。\n",
        "- num_workers=0: 用于数据加载的子进程数量，默认为 0（表示在主进程中加载数据）。\n",
        "\n",
        "**dataloader = DataLoader(...)参数意思:**\n",
        "- dataset: 指定要加载的数据集对象，即前面创建的 GPTDatasetV1 实例。\n",
        "- batch_size=batch_size: 设置每个批次的样本数量。\n",
        "- shuffle=shuffle: 设置是否打乱数据。\n",
        "- drop_last=drop_last: 设置是否丢弃最后一个不完整的批次。\n",
        "- num_workers=num_workers: 设置用于数据加载的子进程数量。"
      ],
      "metadata": {
        "id": "YzNTfTonl1JH"
      },
      "id": "YzNTfTonl1JH"
    },
    {
      "cell_type": "markdown",
      "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd",
      "metadata": {
        "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd"
      },
      "source": [
        "- 让我们使用批次大小为1、上下文大小为4的设置，测试数据加载器在大语言模型（LLM）中的表现。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "df31d96c-6bfd-4564-a956-6192242d7579",
      "metadata": {
        "id": "df31d96c-6bfd-4564-a956-6192242d7579"
      },
      "outputs": [],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
      "metadata": {
        "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
        "outputId": "338a4d30-624d-488a-f41f-6d71146f4925",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(#raw_text 中创建一个数据加载器 但是所批次\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)#将 dataloader 转换为 Python 迭代器，以通过 Python 内置的 next()函数获取下一个条目\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
      "metadata": {
        "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
        "outputId": "127500c5-5a4c-4ac2-850b-2e0d0dd2ee83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ],
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b006212f-de45-468d-bdee-5806216d1679",
      "metadata": {
        "id": "b006212f-de45-468d-bdee-5806216d1679"
      },
      "source": [
        "- 下面是一个示例，步幅等于上下文长度（此处为4）："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
      "metadata": {
        "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16",
      "metadata": {
        "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16"
      },
      "source": [
        "- 我们还可以批量输出。\n",
        "- 因为过多的重叠可能导致过拟合,这里增加了步幅。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了更直观地了解数据加载器的工作原理，请尝试使用不同的参数设置来运行它，比如max_length=2, stride=2 和 max_length=8, stride=2"
      ],
      "metadata": {
        "id": "mMXvYE7XppYy"
      },
      "id": "mMXvYE7XppYy"
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=2, stride=2, shuffle=False\n",
        ")\n",
        "\n",
        "for i, batch in enumerate(dataloader):\n",
        "    print(f\"Batch {i}: {batch}\")\n",
        "    if i ==5:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBGBqanKpjwF",
        "outputId": "454100c1-c5ae-4ae1-d363-a7225d935819"
      },
      "id": "PBGBqanKpjwF",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: [tensor([[ 40, 367]]), tensor([[ 367, 2885]])]\n",
            "Batch 1: [tensor([[2885, 1464]]), tensor([[1464, 1807]])]\n",
            "Batch 2: [tensor([[1807, 3619]]), tensor([[3619,  402]])]\n",
            "Batch 3: [tensor([[402, 271]]), tensor([[  271, 10899]])]\n",
            "Batch 4: [tensor([[10899,  2138]]), tensor([[2138,  257]])]\n",
            "Batch 5: [tensor([[ 257, 7026]]), tensor([[ 7026, 15632]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=8, stride=2, shuffle=False\n",
        ")\n",
        "\n",
        "for i, batch in enumerate(dataloader):\n",
        "    print(f\"Batch {i}: {batch}\")\n",
        "    if i ==5:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smlspCgSpwbf",
        "outputId": "f0f59964-b2a0-4cc8-a000-90f7f333011c"
      },
      "id": "smlspCgSpwbf",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: [tensor([[  40,  367, 2885, 1464, 1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899]])]\n",
            "Batch 1: [tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n",
            "Batch 2: [tensor([[ 1807,  3619,   402,   271, 10899,  2138,   257,  7026]]), tensor([[ 3619,   402,   271, 10899,  2138,   257,  7026, 15632]])]\n",
            "Batch 3: [tensor([[  402,   271, 10899,  2138,   257,  7026, 15632,   438]]), tensor([[  271, 10899,  2138,   257,  7026, 15632,   438,  2016]])]\n",
            "Batch 4: [tensor([[10899,  2138,   257,  7026, 15632,   438,  2016,   257]]), tensor([[ 2138,   257,  7026, 15632,   438,  2016,   257,   922]])]\n",
            "Batch 5: [tensor([[  257,  7026, 15632,   438,  2016,   257,   922,  5891]]), tensor([[ 7026, 15632,   438,  2016,   257,   922,  5891,  1576]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
      "metadata": {
        "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
        "outputId": "89ab2a04-1f24-486d-c5ae-629b3a80b93a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
            "        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899],\n",
            "        [ 3619,   402,   271, 10899,  2138,   257,  7026, 15632]])\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=8, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs1, targets1 = next(data_iter)\n",
        "print(\"2nd Inputs:\\n\", inputs1)\n",
        "print(\"\\n 2nd Targets:\\n\", targets1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvUrod_tqPlm",
        "outputId": "34b48671-7283-41b4-9baa-77b548505079"
      },
      "id": "IvUrod_tqPlm",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2nd Inputs:\n",
            " tensor([[10899,  2138,   257,  7026, 15632,   438,  2016,   257],\n",
            "        [15632,   438,  2016,   257,   922,  5891,  1576,   438]])\n",
            "\n",
            " 2nd Targets:\n",
            " tensor([[ 2138,   257,  7026, 15632,   438,  2016,   257,   922],\n",
            "        [  438,  2016,   257,   922,  5891,  1576,   438,   568]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1",
      "metadata": {
        "id": "2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1"
      },
      "source": [
        "## 2.7 创建标记嵌入(Creating token embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a301068-6ab2-44ff-a915-1ba11688274f",
      "metadata": {
        "id": "1a301068-6ab2-44ff-a915-1ba11688274f"
      },
      "source": [
        "- 数据库快被准备好用于大语言模型（LLM）。\n",
        "- 最后，让我们使用嵌入层将标记转换为连续的向量表示。\n",
        "- 通常，这些嵌入层是大语言模型的一部分，并在模型训练过程中进行更新（训练）。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85089aa-8671-4e5f-a2b3-ef252004ee4c",
      "metadata": {
        "id": "e85089aa-8671-4e5f-a2b3-ef252004ee4c"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"400px\">\n",
        "\n",
        "大语言模型的输入文本的准备工作包括文本分词、将词元转换为词元 ID，以及将词元ID 转换为嵌入向量。本节将利用此前生成的词元 ID 来创建词元嵌入向量"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "由于类 GPT 大语言模型是使用**反向传播算法**（backpropagation algorithm）训练的深度神经网络，因此需要连续的向量表示或嵌入"
      ],
      "metadata": {
        "id": "tkVvEsVOrnS9"
      },
      "id": "tkVvEsVOrnS9"
    },
    {
      "cell_type": "markdown",
      "id": "44e014ca-1fc5-4b90-b6fa-c2097bb92c0b",
      "metadata": {
        "id": "44e014ca-1fc5-4b90-b6fa-c2097bb92c0b"
      },
      "source": [
        "- 假设我们有以下四个输入示例，经过分词后对应的输入 ID 为 2、3、5 和 1："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "15a6304c-9474-4470-b85d-3991a49fa653",
      "metadata": {
        "id": "15a6304c-9474-4470-b85d-3991a49fa653"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])#要加入2,3,5,1的字符"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14da6344-2c71-4837-858d-dd120005ba05",
      "metadata": {
        "id": "14da6344-2c71-4837-858d-dd120005ba05"
      },
      "source": [
        "- 为了简化问题，假设我们只有一个包含 6 个词的小型词汇表，并且我们希望创建大小为 3 的嵌入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "93cb2cee-9aa6-4bb8-8977-c65661d16eda",
      "metadata": {
        "id": "93cb2cee-9aa6-4bb8-8977-c65661d16eda"
      },
      "outputs": [],
      "source": [
        "vocab_size = 6#嵌入层需要支持的唯一标记的总数\n",
        "output_dim = 3#嵌入向量的维度\n",
        "\n",
        "torch.manual_seed(123)#用于设置随机数生成器的种子，确保结果的可复现性\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)#每行表示一个标记的嵌入向量。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ff241f6-78eb-4e4a-a55f-5b2b6196d5b0",
      "metadata": {
        "id": "4ff241f6-78eb-4e4a-a55f-5b2b6196d5b0"
      },
      "source": [
        "- 结果是个6*3的矩阵"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "a686eb61-e737-4351-8f1c-222913d47468",
      "metadata": {
        "id": "a686eb61-e737-4351-8f1c-222913d47468",
        "outputId": "568fa5f4-7236-48cc-96e8-b35106860b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer.weight) #其中每一行对应词汇表中的一个词元，每一列则对应一个嵌入维度"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fcf4f5-0801-4eb4-bb90-acce87935ac7",
      "metadata": {
        "id": "26fcf4f5-0801-4eb4-bb90-acce87935ac7"
      },
      "source": [
        "- 对于熟悉（one-hot encoding）的人来说，上述嵌入层方法本质上只是实现one-hot编码后接矩阵乘法的更高效方式，具体实现可以参考[./embedding_vs_matmul](../03_bonus_embedding-vs-matmul)中的补充代码。\n",
        "- 嵌入层只是对one-hot encoding和矩阵乘法方法的高效实现，因此可以将其视为一个神经网络层，并通过反向传播进行优化。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b0d58c3-83c0-4205-aca2-9c48b19fd4a7",
      "metadata": {
        "id": "4b0d58c3-83c0-4205-aca2-9c48b19fd4a7"
      },
      "source": [
        "- 通过下列操作,我们可以把id3映射到一个三纬的矩阵"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "e43600ba-f287-4746-8ddf-d0f71a9023ca",
      "metadata": {
        "id": "e43600ba-f287-4746-8ddf-d0f71a9023ca",
        "outputId": "4e1a163b-62bf-4850-9d75-a422ceabf39e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(torch.tensor([3]))) #现在将其应用到一个词元 ID 上，以获取嵌入向量"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bbf625-4f36-491d-87b4-3969efb784b0",
      "metadata": {
        "id": "a7bbf625-4f36-491d-87b4-3969efb784b0"
      },
      "source": [
        "- 请注意，上述内容是 `embedding_layer` 权重矩阵中的第四行。\n",
        "- 要嵌入上述所有四个 `input_ids` 值，我们执行以下操作："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "50280ead-0363-44c8-8c35-bb885d92c8b7",
      "metadata": {
        "id": "50280ead-0363-44c8-8c35-bb885d92c8b7",
        "outputId": "ccb0060a-43ae-4875-fd4b-5b73b061ea12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(input_ids)) #打印的输出显示，结果是一个 4×3 的矩阵"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be97ced4-bd13-42b7-866a-4d699a17e155",
      "metadata": {
        "id": "be97ced4-bd13-42b7-866a-4d699a17e155"
      },
      "source": [
        "- 嵌入层本质上是一个查找操作："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f33c2741-bf1b-4c60-b7fd-61409d556646",
      "metadata": {
        "id": "f33c2741-bf1b-4c60-b7fd-61409d556646"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08218d9f-aa1a-4afb-a105-72ff96a54e73",
      "metadata": {
        "id": "08218d9f-aa1a-4afb-a105-72ff96a54e73"
      },
      "source": [
        "- **您可能对比较嵌入层与常规线性层的附加内容感兴趣：[../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyfPztBKxPFG",
        "outputId": "5b1011e1-350d-4413-84a9-5efda4e0f560"
      },
      "id": "SyfPztBKxPFG",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`embedding_layer = torch.nn.Embedding(vocab_size, output_dim)` 这行代码创建了一个 PyTorch 的嵌入层。\n",
        "\n",
        "- `torch.nn.Embedding`: 这是 PyTorch 中用于创建嵌入查找表的类。\n",
        "- `vocab_size`: 表示词汇表的大小，也就是嵌入层需要支持的唯一词元（token）的总数。在这个例子中，`vocab_size = 6`，表示词汇表中有 6 个不同的词元。\n",
        "- `output_dim`: 表示每个词元将被映射到的嵌入向量的维度。在这个例子中，`output_dim = 3`，表示每个词元将由一个 3 维的向量表示。\n",
        "\n",
        "所以，`embedding_layer = torch.nn.Embedding(vocab_size, output_dim)` 创建了一个嵌入层，它可以将词汇表中 `vocab_size` 个不同的词元映射到 `output_dim` 维的向量空间中。\n",
        "\n",
        "`embedding_layer.weight` 是什么意思？\n",
        "\n",
        "- `embedding_layer.weight`: 这是嵌入层内部的一个可学习参数，它是一个形状为 `(vocab_size, output_dim)` 的权重矩阵。这个矩阵的每一行对应词汇表中的一个词元，每一列则对应嵌入向量的一个维度。\n",
        "- 当您打印 `embedding_layer.weight` 时，您看到的就是这个权重矩阵的当前值。这些值在模型训练过程中会通过反向传播进行更新和优化，以便学习到更好的词元表示。\n",
        "\n",
        "`embedding_layer(torch.tensor([3]))` 是什么意思？\n",
        "\n",
        "- `embedding_layer(...)`: 当您像函数一样调用 `embedding_layer` 并传入一个包含词元 ID 的 PyTorch 张量时，嵌入层会执行一个查找操作。\n",
        "- `torch.tensor([3])`: 这是一个包含单个词元 ID `3` 的 PyTorch 张量。\n",
        "- `embedding_layer(torch.tensor([3]))`: 这表示您想查找词汇表中 ID 为 `3` 的词元对应的嵌入向量。\n",
        "- 输出结果是一个形状为 `(1, output_dim)` 的张量，其中 `1` 表示输入的批次大小（这里是单个词元），`output_dim` 是嵌入向量的维度。这个张量包含了 ID 为 `3` 的词元在 `embedding_layer.weight` 矩阵中对应的行向量。\n",
        "\n",
        "本质上，`embedding_layer` 的调用操作就是根据输入的词元 ID，在 `embedding_layer.weight` 矩阵中查找并取出对应的行向量，作为该词元的嵌入表示。这是一种高效的查找操作，可以视为 one-hot 编码后与权重矩阵相乘的简化和优化。\n",
        "\n",
        "如果您传入一个包含多个词元 ID 的张量，例如 `embedding_layer(torch.tensor([2, 3, 5, 1]))`，嵌入层将返回一个形状为 `(batch_size, sequence_length, output_dim)` 的张量，其中 `batch_size` 是输入 ID 张量的第一个维度大小，`sequence_length` 是输入 ID 张量的第二个维度大小（如果存在），`output_dim` 是嵌入向量的维度。这个张量包含了输入中每个词元对应的嵌入向量。\n",
        "\n",
        "**最后一段的理解**:\n",
        "好的，您在使用 embedding_layer(input_ids).shape 时看到了 torch.Size([4, 3]) 的输出，对我的解释感到困惑是正常的。\n",
        "\n",
        "让我详细解释一下：\n",
        "\n",
        "您观察到的 `torch.Size([4, 3])` 是完全正确的，这与我之前的解释是吻合的。\n",
        "\n",
        "我们回顾一下您使用的代码：\n",
        "\n",
        "```python\n",
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # vocab_size=6, output_dim=3\n",
        "print(embedding_layer(input_ids).shape)\n",
        "在这里：\n",
        "\n",
        "input_ids = torch.tensor([2, 3, 5, 1]) 创建了一个形状为 (4,) 的一维张量。它包含了 4 个词元 ID。\n",
        "当您将这个形状为 (4,) 的一维张量传递给 embedding_layer 时，嵌入层会查找并取出这 4 个 ID 对应的嵌入向量。\n",
        "每个嵌入向量的维度是 output_dim，也就是 3。\n",
        "因此，输出的张量包含了 4 个形状为 (3,) 的向量，它们被堆叠在一起，形成了形状为 (4, 3) 的二维张量。\n",
        "我的解释中提到的形状 (batch_size, sequence_length, output_dim) 是更一般的情况，特别是当您的输入 input_ids 是一个批次（batch）的数据时。\n",
        "\n",
        "想象一下，如果您有一个包含多个句子（或序列）的批次，每个句子都由一系列词元 ID 组成。在这种情况下，您的输入 input_ids 会是一个形状为 (batch_size, sequence_length) 的二维张量，其中：\n",
        "\n",
        "batch_size 是批次中句子的数量。\n",
        "sequence_length 是每个句子的长度（包含的词元数量）。\n",
        "当您将这样一个形状为 (batch_size, sequence_length) 的二维张量传递给嵌入层时，嵌入层将为批次中的每个句子、每个位置的词元 ID 查找对应的嵌入向量。输出的张量将是形状为 (batch_size, sequence_length, output_dim) 的三维张量，其中：\n",
        "\n",
        "第一个维度是 batch_size，对应批次中的每个句子。\n",
        "第二个维度是 sequence_length，对应句子中的每个位置。\n",
        "第三个维度是 output_dim，对应每个词元的嵌入向量维度。\n",
        "在您提供的示例中，您的输入 input_ids 是一个形状为 (4,) 的一维张量。在这种情况下，我们可以将这个一维张量视为一个批次大小为 4，序列长度为 1 的特殊情况，或者更直观地理解为，它是一个包含 4 个独立词元 ID 的序列。嵌入层为这 4 个词元 ID 分别查找了对应的 3 维嵌入向量，所以输出的形状是 (4, 3)。\n",
        "\n",
        "简单来说：\n",
        "\n",
        "如果输入是形状为 (N,) 的一维张量，输出将是形状为 (N, output_dim) 的二维张量。\n",
        "如果输入是形状为 (N, L) 的二维张量（表示一个包含 N 个序列的批次，每个序列长度为 L），输出将是形状为 (N, L, output_dim) 的三维张量。\n",
        "您的 input_ids 是一个一维张量，所以输出是二维的 (4, 3)。我的解释是针对更常见的、包含多个序列的批次输入，所以给出了三维的形状 (batch_size, sequence_length, output_dim)。\n",
        "\n",
        "希望这次解释更清晰了！"
      ],
      "metadata": {
        "id": "SduLy4SbvDg2"
      },
      "id": "SduLy4SbvDg2"
    },
    {
      "cell_type": "markdown",
      "id": "c393d270-b950-4bc8-99ea-97d74f2ea0f6",
      "metadata": {
        "id": "c393d270-b950-4bc8-99ea-97d74f2ea0f6"
      },
      "source": [
        "## 2.8 位置信息编码"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24940068-1099-4698-bdc0-e798515e2902",
      "metadata": {
        "id": "24940068-1099-4698-bdc0-e798515e2902"
      },
      "source": [
        "- 嵌入层将 ID 转换为相同的向量表示"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e0b14a2-f3f3-490e-b513-f262dbcf94fa",
      "metadata": {
        "id": "9e0b14a2-f3f3-490e-b513-f262dbcf94fa"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a7d7fe-38a5-46e6-8db6-b688887b0430",
      "metadata": {
        "id": "92a7d7fe-38a5-46e6-8db6-b688887b0430"
      },
      "source": [
        "- 位置信息与标记向量结合，形成大语言模型的最终输入："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48de37db-d54d-45c4-ab3e-88c0783ad2e4",
      "metadata": {
        "id": "48de37db-d54d-45c4-ab3e-88c0783ad2e4"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f187f87-c1f8-4c2e-8050-350bbb972f55",
      "metadata": {
        "id": "7f187f87-c1f8-4c2e-8050-350bbb972f55"
      },
      "source": [
        "- 字节对编码器的词汇表大小为 50,257：\n",
        "- 假设我们想将输入标记编码为一个 256 维的向量表示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "0b9e344d-03a6-4f2c-b723-67b6a20c5041",
      "metadata": {
        "id": "0b9e344d-03a6-4f2c-b723-67b6a20c5041"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)#映射为tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2654722-24e4-4b0d-a43c-436a461eb70b",
      "metadata": {
        "id": "a2654722-24e4-4b0d-a43c-436a461eb70b"
      },
      "source": [
        "- 如果我们从数据加载器中采样数据，我们将每个批次中的标记嵌入为一个 256 维的向量。\n",
        "- 如果我们有一个批次大小为 8，每个批次包含 4 个标记，这将得到一个 8 x 4 x 256 的张量："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "ad56a263-3d2e-4d91-98bf-d0b68d3c7fc3",
      "metadata": {
        "id": "ad56a263-3d2e-4d91-98bf-d0b68d3c7fc3"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "84416b60-3707-4370-bcbc-da0b62f2b64d",
      "metadata": {
        "id": "84416b60-3707-4370-bcbc-da0b62f2b64d",
        "outputId": "0e54b56b-ce8f-4800-9e1d-1ca62fe96836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "7766ec38-30d0-4128-8c31-f49f063c43d1",
      "metadata": {
        "id": "7766ec38-30d0-4128-8c31-f49f063c43d1",
        "outputId": "0e4f6679-e7c4-46ef-8490-4057ac3c91b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)#调用token_embedding_layer将输入inputs映射为对应的嵌入向量。\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2ae164-6f19-4e32-b9e5-76950fcf1c9f",
      "metadata": {
        "id": "fe2ae164-6f19-4e32-b9e5-76950fcf1c9f"
      },
      "source": [
        "- GPT-2 使用绝对位置嵌入，因此我们只需要创建另一个位置嵌入层："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "cc048e20-7ac8-417e-81f5-8fe6f9a4fe07",
      "metadata": {
        "id": "cc048e20-7ac8-417e-81f5-8fe6f9a4fe07"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "#目的是为输入序列中的每个位置生成一个向量,表明位置信息"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9056e371",
      "metadata": {
        "id": "9056e371"
      },
      "source": [
        "- 嵌入层本质上是一个查找表,大小为(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "c369a1e7-d566-4b53-b398-d6adafb44105",
      "metadata": {
        "scrolled": true,
        "id": "c369a1e7-d566-4b53-b398-d6adafb44105",
        "outputId": "b6e4becd-0c20-48a3-c1f3-e1ca026284f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))#生成一个连续整数的1D tensor\n",
        "\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870e9d9f-2935-461a-9518-6d1386b976d6",
      "metadata": {
        "id": "870e9d9f-2935-461a-9518-6d1386b976d6"
      },
      "source": [
        "- 为了创建大语言模型（LLM）中使用的输入嵌入，我们只需将标记嵌入和位置嵌入相加："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "b22fab89-526e-43c8-9035-5b7018e34288",
      "metadata": {
        "id": "b22fab89-526e-43c8-9035-5b7018e34288",
        "outputId": "d09887df-1420-404e-acc2-049796e36e18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings#特征是词语信息跟位置信息的结合\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fbda581-6f9b-476f-8ea7-d244e6a4eaec",
      "metadata": {
        "id": "1fbda581-6f9b-476f-8ea7-d244e6a4eaec"
      },
      "source": [
        "- 在输入处理流程的初始阶段，输入文本被分割为独立的标记。\n",
        "- 在此分割后，这些标记根据预定义的单词表转换为标记 ID："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1bb0f7e-460d-44db-b366-096adcd84fff",
      "metadata": {
        "id": "d1bb0f7e-460d-44db-b366-096adcd84fff"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63230f2e-258f-4497-9e2e-8deee4530364",
      "metadata": {
        "id": "63230f2e-258f-4497-9e2e-8deee4530364"
      },
      "source": [
        "# 总结与收获"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3293a6-45a5-47cd-aa00-b23e3ca0a73f",
      "metadata": {
        "id": "8b3293a6-45a5-47cd-aa00-b23e3ca0a73f"
      },
      "source": [
        "请参见 [./dataloader.ipynb](./dataloader.ipynb) 代码笔记本，这是我们在本章中实现的数据加载器的简洁版，并将在后续章节中用于训练 GPT 模型。\n",
        "\n",
        "请参见 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 获取习题解答。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}